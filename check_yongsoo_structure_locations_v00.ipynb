{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/dtward/data/csh_data/emlddmm')\n",
    "import emlddmm\n",
    "import csv\n",
    "from skimage.measure import marching_cubes\n",
    "from glob import glob\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from os.path import split,join,splitext\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_name = '/nafs/dtward/dong/upenn_atlas/atlas_info_KimRef_FPbasedLabel_v2.7.csv'\n",
    "seg_name = '/nafs/dtward/dong/upenn_atlas/UPenn_labels_reoriented_origin.vtk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87878c41-8dbc-4d0e-9cd9-5d77d4fcb28f",
   "metadata": {},
   "source": [
    "### Load UPenn ontology + Generate lists of descendents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xS,S,_,_ = emlddmm.read_data(seg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad692ab1-c529-47e9-a07b-911a97413197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms xS into a set of coordinates\n",
    "XS = np.stack(np.meshgrid(*xS,indexing='ij'),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba5cc1-f8ac-44ac-ae72-eff190026cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center of mass of region 2294 (Definition of the 1st moment of the indicator function for label 2294)\n",
    "# np.sum(XS*(S[0,...,None]==2294))/np.sum(S==2294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_column = 7 # 8 for allen, 7 for yongsoo\n",
    "label_column = 0 # 0 for both\n",
    "shortname_column = 2# 3 for allen, 2 for yongsoo\n",
    "longname_column = 1# 2 for allen, 1 for yongsoo\n",
    "ontology = dict()\n",
    "with open(ontology_name) as f:\n",
    "    csvreader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    count = 0\n",
    "    for row in csvreader:        \n",
    "        if count == 0:\n",
    "            headers = row\n",
    "            print(headers)\n",
    "        else:\n",
    "            if not row[parent_column]:\n",
    "                parent = -1\n",
    "            else:\n",
    "                parent = int(row[parent_column])\n",
    "            ontology[int(row[label_column])] = (row[shortname_column],row[longname_column],parent)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92746afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to find all the descendants of a given label\n",
    "# first we'll get children\n",
    "children = dict()\n",
    "for o in ontology:\n",
    "    parent = ontology[o][-1]\n",
    "    if parent not in children:\n",
    "        children[parent] = []\n",
    "    children[parent].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ecb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we go from children to descendents\n",
    "descendents = dict(children)\n",
    "for o in descendents:\n",
    "    for child in descendents[o]:\n",
    "        if child in descendents: # if I don't do this i get a key error 0\n",
    "            descendents[o].extend(descendents[child])\n",
    "descendents[0] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3beb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "descendents_and_self = dict(descendents)\n",
    "for o in ontology:\n",
    "    if o not in descendents_and_self:\n",
    "        descendents_and_self[o] = [o]\n",
    "    else:\n",
    "        descendents_and_self[o].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820bb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc42ac-3d87-4072-a59f-31bfbcbe8011",
   "metadata": {},
   "source": [
    "## Generate Boolean masks for CP, CPr (+CPre), CPi, CPc (+CPce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78115e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the descendents of Caudoputemen- rostral\n",
    "# and caudoputemen rostral- extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e69696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all the structures in the CP\n",
    "cp = list(descendents_and_self[672])\n",
    "Scp = np.zeros_like(S)\n",
    "for l in cp:\n",
    "    Scp = np.logical_or(Scp,S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the structures one level below CP, with extreme's merged in\n",
    "rostral = list(descendents_and_self[2376]) # rostral extreme\n",
    "rostral.extend(list(descendents_and_self[2491])) # rostral \n",
    "rostral = list(dict.fromkeys(rostral))\n",
    "\n",
    "intermediate = list(descendents_and_self[2492]) # intermediate\n",
    "\n",
    "caudal = list(descendents_and_self[2496]) # caudal\n",
    "caudal_ = list(descendents_and_self[2495]) # caudal extreme\n",
    "caudal.extend(caudal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67210fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Srostral = np.zeros_like(S)\n",
    "for l in rostral:\n",
    "    Srostral = np.logical_or(Srostral, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sintermediate = np.zeros_like(S)\n",
    "for l in intermediate:\n",
    "    Sintermediate = np.logical_or(Sintermediate, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558600",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaudal = np.zeros_like(S)\n",
    "for l in caudal:\n",
    "    Scaudal = np.logical_or(Scaudal, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908afa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.sum(Srostral>0,axis=(0,2,3)),label='r')\n",
    "ax.plot(np.sum(Sintermediate>0,axis=(0,2,3)),label='i')\n",
    "ax.plot(np.sum(Scaudal>0,axis=(0,2,3)),label='c')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# add one more level down the tree\n",
    "\n",
    "def generate_region_mask(S, regionID):\n",
    "    self_and_children = list(descendents_and_self[regionID])\n",
    "    region_mask = np.zeros_like(S)\n",
    "    for i in self_and_children:\n",
    "        region_mask = np.logical_or(region_mask, S==i)\n",
    "    return region_mask\n",
    "\n",
    "# === Striatum Dorsal Region (STRd) ===\n",
    "\n",
    "# CP regions\n",
    "Scpr_m   = generate_region_mask(S,2294)\n",
    "Scpr_imd = generate_region_mask(S,2295)\n",
    "Scpr_imv = generate_region_mask(S,2296)\n",
    "Scpr_l   = generate_region_mask(S,2497)\n",
    "Scpi_dm  = generate_region_mask(S,2498)\n",
    "Scpi_vm  = generate_region_mask(S,2500)\n",
    "Scpi_dl  = generate_region_mask(S,2499)\n",
    "Scpi_vl  = generate_region_mask(S,2501)\n",
    "Scpc_d   = generate_region_mask(S,2493)\n",
    "Scpc_i   = generate_region_mask(S,2494)\n",
    "Scpc_v   = generate_region_mask(S,2490)\n",
    "\n",
    "# Non-CP regions\n",
    "S_lss = generate_region_mask(S,2001)\n",
    "S_ast = generate_region_mask(S,2050)\n",
    "\n",
    "# === Striatum Ventral Region (STRv) ===\n",
    "S_acb  = generate_region_mask(S,56)  # Accumbens nucleus\n",
    "S_ipac = generate_region_mask(S,998) # Interstitial nucleus of the posterior limb of the anterior commissure\n",
    "S_tu   = generate_region_mask(S,754) # Olfactory tubercle\n",
    "\n",
    "# === (10/3/23) Striatum adjacent regions ===\n",
    "S_lsx = generate_region_mask(S,275)  # Lateral septal complex\n",
    "S_samy = generate_region_mask(S,278) # Striatum-like amygdalar nuclei\n",
    "S_pal = generate_region_mask(S,803)  # Pallidum\n",
    "\n",
    "# === (10/4/23) Striatum adjacent regions ===\n",
    "S_lv =   generate_region_mask(S,81)  # Lateral ventricle\n",
    "S_cc =   generate_region_mask(S,776) # Corpus callosum\n",
    "S_cl =   generate_region_mask(S,583) # Claustrum\n",
    "S_en =   generate_region_mask(S,942) # Endopiriform nucleus\n",
    "S_pir =  generate_region_mask(S,961) # Piriform nucleus\n",
    "S_ic =   generate_region_mask(S,6)   # Internal capsule\n",
    "S_mfbc = generate_region_mask(S,768) # Cerebrum related (fiber tracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9184617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'N (Scp): {np.sum(Scp):,}')\n",
    "print(f'N (Scpr): {np.sum(Srostral):,}')\n",
    "print(f'N (Scpi): {np.sum(Sintermediate):,}')\n",
    "print(f'N (Scpc): {np.sum(Scaudal):,}')\n",
    "\n",
    "print(f'N (Scpr_m): {np.sum(Scpr_m):,}')\n",
    "print(f'N (Scpr_imd): {np.sum(Scpr_imd):,}')\n",
    "print(f'N (Scpr_imv): {np.sum(Scpr_imv):,}')\n",
    "print(f'N (Scpr_l): {np.sum(Scpr_l):,}')\n",
    "\n",
    "print(f'N (Scpi_dm): {np.sum(Scpi_dm):,}')\n",
    "print(f'N (Scpi_vm): {np.sum(Scpi_vm):,}')\n",
    "print(f'N (Scpi_dl): {np.sum(Scpi_dl):,}')\n",
    "print(f'N (Scpi_vl): {np.sum(Scpi_vl):,}')\n",
    "\n",
    "print(f'N (Scpc_d): {np.sum(Scpc_d):,}')\n",
    "print(f'N (Scpc_i): {np.sum(Scpc_i):,}')\n",
    "print(f'N (Scpc_v): {np.sum(Scpc_v):,}')\n",
    "\n",
    "print(f'N (S_lss): {np.sum(S_lss):,}')\n",
    "print(f'N (S_ast): {np.sum(S_ast):,}')\n",
    "\n",
    "print(f'N (S_acb): {np.sum(S_acb):,}')\n",
    "print(f'N (S_ipac): {np.sum(S_ipac):,}')\n",
    "print(f'N (S_tu): {np.sum(S_tu):,}')\n",
    "\n",
    "print(f'N (S_lsx): {np.sum(S_lsx):,}')\n",
    "print(f'N (S_samy): {np.sum(S_samy):,}')\n",
    "print(f'N (S_pal): {np.sum(S_pal):,}')\n",
    "\n",
    "print(f'N (S_lv): {np.sum(S_lv):,}')\n",
    "print(f'N (S_cc): {np.sum(S_cc):,}')\n",
    "print(f'N (S_cl): {np.sum(S_cl):,}')\n",
    "print(f'N (S_en): {np.sum(S_en):,}')\n",
    "print(f'N (S_pir): {np.sum(S_pir):,}')\n",
    "print(f'N (S_ic): {np.sum(S_ic):,}')\n",
    "print(f'N (S_mfbc): {np.sum(S_mfbc):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2750940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think what I'd like to do is assign a gaussian to each region\n",
    "# then give the neurons a distribution based on the Gaussian\n",
    "# to do this I should load a set of neurons\n",
    "# and also start visualizing\n",
    "\n",
    "# what I think would make sense is to take all the cp structures\n",
    "# blur them a lot\n",
    "# then assign probabilities\n",
    "# we also want to look at the neurons though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to start by visualizing\n",
    "# we need to load swc files\n",
    "# and we need to contour the surfaces\n",
    "d = [x[1] - x[0] for x in xS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "down = 16\n",
    "xSd,Scpd = emlddmm.downmode(xS,Scp[0],down=[down,down,down])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = [x[1] - x[0] for x in xSd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verts,faces,normals,values = marching_cubes(Scpd,level=0.5,spacing=dd)\n",
    "verts = verts + np.array([x[0] for x in xSd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623af01-26ff-4d4a-8906-9ad8f72069c2",
   "metadata": {},
   "source": [
    "## Load SWC files and Display 2x2 figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb54b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# swcdir = '../swc_out_v08' # this is tme07\n",
    "# swcdir = '../dragonfly_tme09-1/swc_out_v08'\n",
    "# swcdir = '/home/abenneck/dragonfly_work/dragonfly_outputs/TME08-1/dragonfly_joint_outputs'\n",
    "\n",
    "brain = 'TME08-1'\n",
    "swcdir = f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs'\n",
    "files = glob(join(swcdir,'*.swc'))\n",
    "files = [f for f in files if 'permuted' not in f]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856d357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for file in files:\n",
    "    with open(file)  as f:\n",
    "        for i,line in enumerate(f):    \n",
    "            print(line)\n",
    "            if 'Tward' in line:                \n",
    "                continue\n",
    "            else:\n",
    "                if ',' in line:\n",
    "                    lim = ','\n",
    "                else:\n",
    "                    lim = ''\n",
    "                coords = [float(c) for c in line.split(lim)[2:5]]                \n",
    "                    \n",
    "                x.append(coords)\n",
    "                break\n",
    "    \n",
    "            \n",
    "x = np.array(x)            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539eeab-c91e-4961-a246-f1c19789e1fd",
   "metadata": {},
   "source": [
    "### Generate + Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "alpha = 0.25\n",
    "lw = 0.25\n",
    "\n",
    "\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,2,1,projection='3d')\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,2,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(0,90)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,3,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(0,0)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,4,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(90,0)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "# fig.suptitle(swcdir.split('/')[-2])\n",
    "# fig.savefig('CP_SWC_QC_figure_'+swcdir.split('/')[-2]+'.jpg')\n",
    "\n",
    "fig.suptitle(f'{brain}')\n",
    "# fig.savefig(join(f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/',f'CP_SWC_QC_figure_{brain}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450082b-f335-46d9-b784-f31d2785314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(f'End of QC figure generation for {brain}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2912c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so what is a plan going forward\n",
    "# I'd like to take every blob as a gaussian\n",
    "# the height is its volume\n",
    "# the mean is its mean\n",
    "# the covariance is its covariance\n",
    "# the only trouble here is the left right issue\n",
    "# another posiblility is to just blur the labels\n",
    "# this may get rid of small structures though\n",
    "# but I could give less blur to the small structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first going forward\n",
    "# 1. make a figure like this to get a sense of the uncertainty for each brain\n",
    "# 2. make a probabilitistic version of the CP structures.\n",
    "# I'd like to model each structure as a gaussian blob (ellispoids with soft boundaries)\n",
    "# this needs three parameters\n",
    "# the mean (a 3 element vector)\n",
    "# the covariance (3x3 symmetric matrix)\n",
    "# and the height/amplitude of the gaussian (one positive number)\n",
    "\n",
    "# The height (amplitude) should be the number of voxels in the structure\n",
    "# the mean, is going to be the first moment of the segmentation\n",
    "# the covariance, is the second central moment of the segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335669b-da7f-4643-a6e9-86df921dbebb",
   "metadata": {},
   "source": [
    "### Compute + Save Gaussian parameters for CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d68113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_gaussian_param(subregion_mask, hemi, fname):\n",
    "    if hemi == 'R':\n",
    "        LR_indicator = (xS[1][:,None]>=0) # Right hemisphere\n",
    "    else:\n",
    "        LR_indicator = (xS[1][:,None]<0)  # Left hemisphere\n",
    "    \n",
    "    # ===== Compute Gaussian parameters =====\n",
    "    \n",
    "    # number of voxels in 'subregion'\n",
    "    Ncp = np.sum(subregion_mask*LR_indicator)\n",
    "    # print(f'number of voxels {Ncp}')\n",
    "    \n",
    "    # note xS tells us the location of each voxel\n",
    "    # let's compute the first moment\n",
    "    mucp = [np.sum(xS[0][:,None,None]*subregion_mask*LR_indicator)/Ncp, np.sum(xS[1][:,None]*subregion_mask*LR_indicator)/Ncp, np.sum(xS[2][:]*subregion_mask*LR_indicator)/Ncp]\n",
    "    # print(f'mu {mucp}')\n",
    "    \n",
    "    # now calculate the covariance matrix\n",
    "    covcp01 = np.sum((xS[0][:,None,None] - mucp[0])*(xS[1][:,None] - mucp[1])*subregion_mask*LR_indicator)/Ncp # here row 0 column 1\n",
    "    covcp02 = np.sum((xS[0][:,None,None] - mucp[0])*(xS[2][None,:] - mucp[2])*subregion_mask*LR_indicator)/Ncp # here row 0 column 2\n",
    "    covcp12 = np.sum((xS[1][None,:,None] - mucp[1])*(xS[2][None,:] - mucp[2])*subregion_mask*LR_indicator)/Ncp # here row 1 column 2\n",
    "    \n",
    "    covcp00 = np.sum(((xS[0][:,None,None] - mucp[0])**2)*subregion_mask*LR_indicator)/Ncp # here row 0 column 0\n",
    "    covcp11 = np.sum(((xS[1][None,:,None] - mucp[1])**2)*subregion_mask*LR_indicator)/Ncp # here row 1 column 1\n",
    "    covcp22 = np.sum(((xS[2][None,None,:] - mucp[2])**2)*subregion_mask*LR_indicator)/Ncp # here row 2 column 2\n",
    "    \n",
    "    covcp = [[covcp00,covcp01,covcp02],[covcp01,covcp11,covcp12],[covcp02,covcp12,covcp22]]\n",
    "    \n",
    "    # print('cov:')\n",
    "    # print(f'{covcp[0]}')\n",
    "    # print(f'{covcp[1]}')\n",
    "    # print(f'{covcp[2]}')\n",
    "    \n",
    "    # Save Gaussian parameters in.npz file with keys [height, mu, cov]\n",
    "    save_path = join('/home/abenneck/dragonfly_work/gaussian_parameters/',fname)\n",
    "    np.savez(save_path,height=[Ncp],mu=mucp,cov=covcp)\n",
    "    print(f'Saved {fname}\\n')\n",
    "\n",
    "genParam = False\n",
    "if genParam:\n",
    "    for hemi in ['L','R']:\n",
    "        generate_gaussian_param(Scp, hemi,f'cp_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Srostral, hemi,f'cpr_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Sintermediate, hemi,f'cpi_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scaudal, hemi,f'cpc_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(Scpr_m, hemi,f'cpr_m_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpr_imd, hemi,f'cpr_imd_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpr_imv, hemi,f'cpr_imv_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpr_l, hemi,f'cpr_l_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(Scpi_dm, hemi,f'cpi_dm_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpi_vm, hemi,f'cpi_vm_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpi_dl, hemi,f'cpi_dl_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpi_vl, hemi,f'cpi_vl_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(Scpc_d, hemi,f'cpc_d_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpc_i, hemi,f'cpc_i_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpc_v, hemi,f'cpc_v_param_{hemi}.npz')\n",
    "\n",
    "        generate_gaussian_param(S_lss, hemi, f'lss_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_ast, hemi, f'ast_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(S_acb, hemi, f'acb_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_ipac, hemi, f'ipac_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_tu, hemi, f'tu_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(S_lsx, hemi, f'lsx_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_samy, hemi, f'samy_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_pal, hemi, f'pal_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(S_lv, hemi,f'lv_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_cc, hemi,f'cc_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_cl, hemi,f'cl_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_en, hemi,f'en_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_pir, hemi,f'pir_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_ic, hemi,f'ic_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_mfbc, hemi,f'mfbc_param_{hemi}.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e9fd0-bade-4cb6-9437-b782271d82e4",
   "metadata": {},
   "source": [
    "### Load Gaussian parameters given hemi and subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959276e-6650-47a0-9930-d6cc08ea2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hemi = 'L'\n",
    "hemi = 'R'\n",
    "subregion = ''\n",
    "\n",
    "fname = f'cpc_v_param_{hemi}.npz'\n",
    "fpath = join(f'/home/abenneck/dragonfly_work/gaussian_parameters/',fname)\n",
    "out = np.load(fpath)\n",
    "\n",
    "h = out['height'][0]\n",
    "mu = out['mu']\n",
    "cov = out['cov']\n",
    "\n",
    "print(f'Displating parameters for {fname};\\n')\n",
    "print(f'height: {h}')\n",
    "print(f'mu: {mu}')\n",
    "print(f'cov: {cov}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have parameters for each gaussian\n",
    "# then we can evalute them at every point in space\n",
    "# so for a given cell body, we can find a distribution over this number of regions\n",
    "# for each level of the tree, you can get a distribution over all the structures in that level\n",
    "# start with R-I-C level\n",
    "# TODO, come up with some measure of concordance\n",
    "#   that is related to nick's labels, but allows some slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't really believe the uncertaint is given by the voxel size (we're not getting 1 voxel accurate registration)\n",
    "# we believe it is related to the anatomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd954b7-a66c-4ba6-a893-99f779adf4f1",
   "metadata": {},
   "source": [
    "### Compute distance transform for each mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a774b-9a97-40c3-bdb3-243d21e06233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt\n",
    "import time\n",
    "\n",
    "def generateDistanceTransform(xS, mask, hemi, fname, save_path = '', scale_factor = 10):\n",
    "    start = time.time()\n",
    "    if hemi == 'R':\n",
    "        LR_indicator = (xS[1][:,None]>=0) # Right hemisphere\n",
    "    else:\n",
    "        LR_indicator = (xS[1][:,None]<0)  # Left hemisphere\n",
    "\n",
    "    if save_path == '':\n",
    "        save_path = '/home/abenneck/dragonfly_work/distance_parameters/'\n",
    "\n",
    "    transformedMask = np.exp(-distance_transform_edt(1-mask*LR_indicator)/scale_factor)\n",
    "    # transformedMask[transformedMask == 1.0] = 0.0\n",
    "    transformedMask = transformedMask / np.sum(transformedMask)\n",
    "    \n",
    "    # Save distance transform parameters in .npz file with keys [mask]\n",
    "    save_path = join(save_path,fname)\n",
    "    np.savez(save_path, mask = transformedMask.astype(np.float32))\n",
    "    print(f'Saved {fname} after {time.time() - start : .2f} s')\n",
    "    \n",
    "    return transformedMask\n",
    "\n",
    "saveDistanceParam = False\n",
    "if saveDistanceParam:\n",
    "    for hemi in ['L','R']:\n",
    "        generateDistanceTransform(xS, Scp, hemi, f'cp_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Srostral, hemi, f'cpr_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Sintermediate, hemi, f'cpi_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scaudal, hemi, f'cpc_dist_param_{hemi}.npz')\n",
    "    \n",
    "        generateDistanceTransform(xS, Scpr_m, hemi, f'cpr_m_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpr_imd, hemi, f'cpr_imd_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpr_imv, hemi, f'cpr_imv_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpr_l, hemi, f'cpr_l_dist_param_{hemi}.npz')\n",
    "    \n",
    "        generateDistanceTransform(xS, Scpi_dm, hemi, f'cpi_dm_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpi_vm, hemi, f'cpi_vm_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpi_dl, hemi, f'cpi_dl_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpi_vl, hemi, f'cpi_vl_dist_param_{hemi}.npz')\n",
    "    \n",
    "        generateDistanceTransform(xS, Scpc_d, hemi, f'cpc_d_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpc_i, hemi, f'cpc_i_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpc_v, hemi, f'cpc_v_dist_param_{hemi}.npz')\n",
    "\n",
    "        generateDistanceTransform(xS, S_lss, hemi, f'lss_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_ast, hemi, f'ast_dist_param_{hemi}.npz')\n",
    "        \n",
    "        generateDistanceTransform(xS, S_acb, hemi, f'acb_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_ipac, hemi, f'ipac_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_tu, hemi, f'tu_dist_param_{hemi}.npz')\n",
    "        \n",
    "        generateDistanceTransform(xS, S_lsx, hemi, f'lsx_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_samy, hemi, f'samy_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_pal, hemi, f'pal_dist_param_{hemi}.npz')\n",
    "\n",
    "        generateDistanceTransform(xS, S_lv, hemi, f'lv_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_cc, hemi, f'cc_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_cl, hemi, f'cl_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_en, hemi, f'en_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_pir, hemi, f'pir_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_ic, hemi, f'ic_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_mfbc, hemi, f'mfbc_dist_param_{hemi}.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb16b5-3732-4bcd-8cba-c7ba5af54908",
   "metadata": {},
   "source": [
    "### Load dist param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06ee5b-9e47-4aaa-8550-49c992cd01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.load('/home/abenneck/dragonfly_work/distance_parameters/cp_dist_param_L.npz')\n",
    "out = out['mask']\n",
    "\n",
    "x, y, z = 100, 100, -100\n",
    "\n",
    "print(out[0, x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dbfa9-d389-4c3b-b307-862cde211804",
   "metadata": {},
   "source": [
    "### Preprocessing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0541ee-04db-4f72-9f78-1771b57d1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_filter_gauss(fileName):\n",
    "    if len(fileName) <= 15 and fileName != '.ipynb_checkpoints' and 'cp' in fileName:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def L2_filter_gauss(fileName):\n",
    "    if len(fileName) > 15 and fileName != '.ipynb_checkpoints' and 'cp' in fileName:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def non_CP_filter_gauss(fileName):\n",
    "    if not L2_filter_gauss(fileName) and not L1_filter_gauss(fileName) and fileName != '.ipynb_checkpoints':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def L1_filter_dist(fileName):\n",
    "    if len(fileName) <= 21 and fileName != '.ipynb_checkpoints' and 'cp' in fileName:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def L2_filter_dist(fileName):\n",
    "    if len(fileName) > 21 and fileName != '.ipynb_checkpoints' and 'cp' in fileName:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def non_CP_filter_dist(fileName):\n",
    "    if not L2_filter_dist(fileName) and not L1_filter_dist(fileName) and fileName != '.ipynb_checkpoints':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_coord_list(region_id, XS, S):\n",
    "    self_and_children = list(descendents_and_self[region_id])\n",
    "    allCoords = list()\n",
    "    for idNum in self_and_children:\n",
    "        regionCoords = XS[S[0]==idNum]\n",
    "        if len(regionCoords) != 0:\n",
    "            for coord in regionCoords:\n",
    "                allCoords.append(coord)\n",
    "    return allCoords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018300b3-6238-4a0b-a80a-796c773ec5e9",
   "metadata": {},
   "source": [
    "### Probability computation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b2cf2-b6fb-4ded-bd1a-5c3f4d07c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance with vectorization\n",
    "def dist_from_cp(allCoords, soma_location, useForLoop = False):\n",
    "    if useForLoop:\n",
    "        minDist = -1\n",
    "        for c in allCoords:\n",
    "            distance = math.dist(c,soma_location)\n",
    "            if minDist == -1 or minDist > distance:\n",
    "                minDist = distance\n",
    "        return minDist        \n",
    "    else:\n",
    "        # out = allCoords-soma_location => [allCoords[0]-soma_location, allCoords[1]-soma_location, ...]\n",
    "        # out = (out)**2                => Square all terms within out\n",
    "        # out = np.sum(out, axis=1)     => [out[0][0]+out[0][1]+out[0][2], out[1][0]+out[1][1]+out[1][2], ...]\n",
    "        # out = np.sqrt(out)            => sqrt all terms within out\n",
    "        # out = np.min(out)             => min of all distances\n",
    "        return np.min(np.sqrt(np.sum((allCoords-soma_location)**2, axis=1)))\n",
    "\n",
    "# Round neuron coord to nearest atlas coord\n",
    "def roundCoord(sub_xS, coord):\n",
    "    if coord < np.min(sub_xS):\n",
    "        coord = np.min(sub_xS)\n",
    "    elif coord > np.max(sub_xS):\n",
    "        coord = np.max(sub_xS)\n",
    "    else:\n",
    "        coord_rounded = 20*round(coord/20) # Round coordinate to nearest 20\n",
    "        if coord < 0:\n",
    "            if coord - coord_rounded >= 0:\n",
    "                coord = coord_rounded + 10\n",
    "            else:\n",
    "                coord = coord_rounded - 10\n",
    "        else:\n",
    "            if coord - coord_rounded >= 0:\n",
    "                coord = coord_rounded + 10\n",
    "            else:\n",
    "                coord = coord_rounded - 10\n",
    "    return coord\n",
    "\n",
    "def append_nonCP_regions(allProb):\n",
    "    \n",
    "    allProb.insert(allProb.columns.get_loc('CP_R')+1,'ACB0_L',allProb['ACB_L'])\n",
    "    allProb.insert(allProb.columns.get_loc('ACB0_L')+1,'ACB0_R',allProb['ACB_R'])\n",
    "    \n",
    "    allProb.insert(allProb.columns.get_loc('CPr_R')+1,'ACB1_L',allProb['ACB_L'])\n",
    "    allProb.insert(allProb.columns.get_loc('ACB1_L')+1,'ACB1_R',allProb['ACB_R'])\n",
    "    \n",
    "    allProb.insert(allProb.columns.get_loc('CPr_m_R')+1,'ACB2_L',allProb['ACB_L'])\n",
    "    allProb.insert(allProb.columns.get_loc('ACB2_L')+1,'ACB2_R',allProb['ACB_R'])\n",
    "    \n",
    "    return allProb    \n",
    "\n",
    "# Absolute probabilities => (Normalize probabilities at multiple levels) => Conditional probabilities\n",
    "def normalize_prob(allProb):    \n",
    "    L0_labels = ['CP_L','CP_R','ACB0_L','ACB0_R']\n",
    "    allProb[L0_labels] = allProb[L0_labels].div(np.sum(allProb[L0_labels], axis=1), axis=0)\n",
    "\n",
    "    L1_labels = ['CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R','ACB1_L','ACB1_R']\n",
    "    allProb[L1_labels] = allProb[L1_labels].div(np.sum(allProb[L1_labels], axis=1), axis=0)\n",
    "    \n",
    "    L2_labels = ['CPc_d_L','CPc_d_R','CPc_i_L','CPc_i_R','CPc_v_L','CPc_v_R','CPi_dl_L','CPi_dl_R','CPi_dm_L','CPi_dm_R','CPi_vl_L','CPi_vl_R','CPi_vm_L','CPi_vm_R','CPr_imd_L','CPr_imd_R','CPr_imv_L','CPr_imv_R','CPr_l_L','CPr_l_R','CPr_m_L','CPr_m_R','ACB2_L','ACB2_R']\n",
    "    allProb[L2_labels] = allProb[L2_labels].div(np.sum(allProb[L2_labels], axis=1), axis=0)\n",
    "\n",
    "    nonCP_labels = ['ACB_L','ACB_R','AST_L','AST_R','CC_L','CC_R','CL_L','CL_R','EN_L','EN_R','IC_L','IC_R','IPAC_L','IPAC_R','LSS_L','LSS_R','LSX_L','LSX_R','LV_L','LV_R','MFBC_L','MFBC_R','PAL_L','PAL_R','PIR_L','PIR_R','SAMY_L','SAMY_R','TU_L','TU_R']\n",
    "    allProb[nonCP_labels] = allProb[nonCP_labels].div(np.sum(allProb[nonCP_labels], axis=1), axis=0)\n",
    "\n",
    "    return allProb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e8aef-a09f-4fd6-a454-6210d4511fc7",
   "metadata": {},
   "source": [
    "### Postprocessing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199635b4-6038-478e-b937-259b139c3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to remove the .ipynb_checkpoints file from lists\n",
    "def brain_filter(dirName):\n",
    "    if dirName == '.ipynb_checkpoints' or dirName == 'TME20-1':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Return a pd.DataFrame (1 x len (df)) of the column name in df containing the corresponding value in vals for each row of df\n",
    "def xlookup(vals, df, cName = ''):\n",
    "    out = list()\n",
    "    for row in df.iterrows():\n",
    "        idx, row = row\n",
    "        val = vals[idx]\n",
    "        colName = row[row == val].keys()[0]\n",
    "        out.append(colName)\n",
    "    return pd.DataFrame(data=out, columns = [cName])\n",
    "\n",
    "# Return a pd.Series (1 x len(df)) of the nth largest value in each row of df\n",
    "def nmax(df, n):\n",
    "    out = list()\n",
    "    for row in df.iterrows():\n",
    "        idx, row = row\n",
    "        nth_largest = sorted(row, reverse=True)[n-1]\n",
    "        out.append(nth_largest)\n",
    "    return pd.Series(data=out)\n",
    "\n",
    "def compute_top_n_maximums(data, all_labels, n=3):\n",
    "    \n",
    "    for i, labels in enumerate(all_labels):\n",
    "        L0_df = data[labels]\n",
    "        \n",
    "        L0_P1 = L0_df.max(axis=1)\n",
    "        L0_1 = xlookup(L0_P1, L0_df)\n",
    "    \n",
    "        if i == 0:\n",
    "            leftmost_col_name = 'fname'\n",
    "        else:\n",
    "            leftmost_col_name = f'P(L{i-1},3)'\n",
    "        \n",
    "        data.insert(data.columns.get_loc(leftmost_col_name)+1,f'L{i} (1)',L0_1)\n",
    "        data.insert(data.columns.get_loc(f'L{i} (1)')+1,f'P(L{i},1)',L0_P1)\n",
    "        \n",
    "        L0_P2 = nmax(L0_df, 2)\n",
    "        L0_2 = xlookup(L0_P2, L0_df)\n",
    "        data.insert(data.columns.get_loc(f'P(L{i},1)')+1,f'L{i} (2)',L0_2)\n",
    "        data.insert(data.columns.get_loc(f'L{i} (2)')+1,f'P(L{i},2)',L0_P2)\n",
    "        \n",
    "        L0_P3 = nmax(L0_df,3)\n",
    "        L0_3 = xlookup(L0_P3, L0_df)\n",
    "        data.insert(data.columns.get_loc(f'P(L{i},2)')+1,f'L{i} (3)',L0_3)\n",
    "        data.insert(data.columns.get_loc(f'L{i} (3)')+1,f'P(L{i},3)',L0_P3)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c2dea-2f27-4886-9d0b-70f7895d36d6",
   "metadata": {},
   "source": [
    "## Generate probability distributions, assign labels to neurons, and save csv files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cdf9c-f49f-4caa-87ee-b1b8184e865e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# brains = sorted(filter(brain_filter, os.listdir('/home/abenneck/dragonfly_work/dragonfly_outputs/')))\n",
    "brains = ['TME07-1', 'TME09-1', 'TME10-1', 'TME10-3', 'TME12-1']\n",
    "# brains = ['TME08-1']\n",
    "\n",
    "allRegionCoords = get_coord_list(672, XS, S) # 672 is regionID for CP\n",
    "allRegionCoords = np.asarray(allRegionCoords)\n",
    "uniform_prior = 1/100\n",
    "\n",
    "print('Loading regional amplitudes')\n",
    "start = time.time()\n",
    "# Create list of priors from presaved parameters\n",
    "paramDir = '/home/abenneck/dragonfly_work/gaussian_parameters'\n",
    "L1_files = sorted(filter(L1_filter_gauss,os.listdir(paramDir)))\n",
    "L2_files = sorted(filter(L2_filter_gauss,os.listdir(paramDir)))\n",
    "nonCP_files = sorted(filter(non_CP_filter_gauss,os.listdir(paramDir)))\n",
    "allPriors = list()\n",
    "for fileList in [L1_files, L2_files, nonCP_files]:\n",
    "    for file in fileList:\n",
    "        data = np.load(os.path.join(paramDir,file))\n",
    "        norm_factor = data['height'].item()\n",
    "        allPriors.append(norm_factor)\n",
    "print(f'Finished loading amp in {time.time() - start} s\\n')\n",
    "\n",
    "print('Loading distance transform distributions')\n",
    "start = time.time()\n",
    "# Create list of distributions from presaved distance transforms\n",
    "# paramDir = '/home/abenneck/dragonfly_work/distance_parameters'\n",
    "paramDir = '/home/abenneck/dragonfly_work/distance_parameters'\n",
    "L1_files = sorted(filter(L1_filter_dist,os.listdir(paramDir)))\n",
    "L2_files = sorted(filter(L2_filter_dist,os.listdir(paramDir)))\n",
    "nonCP_files = sorted(filter(non_CP_filter_dist,os.listdir(paramDir)))\n",
    "allTransforms = list()\n",
    "for fileList in [L1_files, L2_files, nonCP_files]:\n",
    "    for file in fileList:\n",
    "        data = np.load(os.path.join(paramDir,file))\n",
    "        probDist = data['mask']\n",
    "        allTransforms.append(probDist)\n",
    "print(f'Finished loading distributions in {time.time() - start} s\\n')\n",
    "\n",
    "# if True:\n",
    "#     raise Exception('Rewrite some of the below code')\n",
    "\n",
    "print('Generating probabilities')\n",
    "start = time.time()\n",
    "# Generate probabilities\n",
    "for brain in brains:\n",
    "    # Define relevant directories\n",
    "    neuronDir = f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs/'\n",
    "\n",
    "    # Note: The ACB#_x columns are inserted after computing absolute probabilities, but before normalization\n",
    "    L0_col = ['mouse ID', 'slice', 'hemi', 'neuron ID','dist (CP)','fname','CP_L','CP_R']\n",
    "    L1_col = ['CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R',]\n",
    "    L2_col = ['CPc_d_L','CPc_d_R','CPc_i_L','CPc_i_R','CPc_v_L','CPc_v_R','CPi_dl_L','CPi_dl_R','CPi_dm_L','CPi_dm_R','CPi_vl_L','CPi_vl_R','CPi_vm_L','CPi_vm_R','CPr_imd_L','CPr_imd_R','CPr_imv_L','CPr_imv_R','CPr_l_L','CPr_l_R','CPr_m_L','CPr_m_R']\n",
    "    nonCP_col = ['ACB_L','ACB_R','AST_L','AST_R','CC_L','CC_R','CL_L','CL_R','EN_L','EN_R','IC_L','IC_R','IPAC_L','IPAC_R','LSS_L','LSS_R','LSX_L','LSX_R','LV_L','LV_R','MFBC_L','MFBC_R','PAL_L','PAL_R','PIR_L','PIR_R','SAMY_L','SAMY_R','TU_L','TU_R']\n",
    "    allCol = np.concatenate([L0_col, L1_col, L2_col, nonCP_col])\n",
    "    allProb = pd.DataFrame(columns=allCol)\n",
    "    \n",
    "    # Generate regional probabilities for every neuron in neuronDir\n",
    "    row_idx = 0\n",
    "    for i, file in enumerate(sorted(os.listdir(neuronDir))):\n",
    "        if \"mapped.swc\" in file:\n",
    "            # Load soma coordinates\n",
    "            data = pd.read_csv(os.path.join(neuronDir,file))\n",
    "            x = float(data.columns[2])\n",
    "            y = float(data.columns[3])\n",
    "            z = float(data.columns[4])\n",
    "            soma_location = [x,y,z]\n",
    "\n",
    "            # Get idx for soma location in xS\n",
    "            x_ind = np.where(xS[0] == roundCoord(xS[0], x))[0].item()\n",
    "            y_ind = np.where(xS[1] == roundCoord(xS[1], y))[0].item() \n",
    "            z_ind = np.where(xS[2] == roundCoord(xS[2], z))[0].item()\n",
    "\n",
    "            # Get metadata from filename\n",
    "            mouseID = file.split('_')[1]\n",
    "            sliceID = file.split('_')[4][:-1]\n",
    "            hemiID = file.split('_')[4][-1]\n",
    "            neuronID = file.split('_')[0]\n",
    "    \n",
    "            # Compute probabilities from distance transform probabilities\n",
    "            cpDist = dist_from_cp(allRegionCoords,soma_location)\n",
    "            neuronProb = [mouseID, sliceID, hemiID, neuronID, cpDist, file]\n",
    "            for j, prior in enumerate(allPriors):\n",
    "                neuronProb.append(uniform_prior*allTransforms[j][0, x_ind, y_ind, z_ind])\n",
    "\n",
    "            allProb.loc[row_idx] = neuronProb\n",
    "            row_idx+=1\n",
    "            print(f'{i}/{len(sorted(os.listdir(neuronDir)))} Finished {file}\\n')\n",
    "\n",
    "    # Insert ACB at each level\n",
    "    allProb = append_nonCP_regions(allProb)\n",
    "\n",
    "    # Normalize probabilities at each level\n",
    "    allProb = normalize_prob(allProb)\n",
    "\n",
    "    # Identify top 3 regions for each neuron and insert columns into allProb\n",
    "    L0_labels = ['CP_L','CP_R','ACB0_L','ACB0_R']\n",
    "    L1_labels = ['CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R','ACB1_L','ACB1_R']\n",
    "    L2_labels = ['CPc_d_L','CPc_d_R','CPc_i_L','CPc_i_R','CPc_v_L','CPc_v_R','CPi_dl_L','CPi_dl_R','CPi_dm_L','CPi_dm_R','CPi_vl_L','CPi_vl_R','CPi_vm_L','CPi_vm_R','CPr_imd_L','CPr_imd_R','CPr_imv_L','CPr_imv_R','CPr_l_L','CPr_l_R','CPr_m_L','CPr_m_R','ACB2_L','ACB2_R']\n",
    "    all_labels = [L0_labels, L1_labels, L2_labels]\n",
    "\n",
    "    allProb = compute_top_n_maximums(allProb, all_labels)\n",
    "    \n",
    "    # Save probability distributions as one .csv file in tempDir\n",
    "    tempDir = f'/home/abenneck/dragonfly_work/{brain}_neuron_region_prob.csv'   \n",
    "    allProb.to_csv(tempDir, index=False)\n",
    "\n",
    "    print(f'Finished {brain} in {time.time() - start} s with an average of {(time.time() - start) / len(allProb)} s / neuron\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7432418-ab11-4894-abf2-5c67d1b55814",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('End of postprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a0bc3-795c-44c4-a10e-93f6850df784",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/abenneck/dragonfly_work/TME08-1_neuron_region_prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b030d-bfe1-4782-9961-323ebe6388e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create list of multivariate Gaussian RVs from presaved parameters\n",
    "# allDist = list()\n",
    "# for fileList in [L1_files, L2_files]:\n",
    "#     for file in fileList:\n",
    "#         data = np.load(os.path.join(paramDir,file))\n",
    "#         norm_factor = data['height'].item()\n",
    "#         mu = data['mu']\n",
    "#         cov = data['cov']\n",
    "#         regionDist = multivariate_normal(mean=mu, cov=cov, allow_singular=False)\n",
    "#         # regionDist = \n",
    "#         allDist.append([regionDist,norm_factor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2874d-1086-49f4-88c5-160275920345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_columns', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
