{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/dtward/data/csh_data/emlddmm')\n",
    "import emlddmm\n",
    "import csv\n",
    "from skimage.measure import marching_cubes\n",
    "from glob import glob\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from os.path import split,join,splitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_name = '/nafs/dtward/dong/upenn_atlas/atlas_info_KimRef_FPbasedLabel_v2.7.csv'\n",
    "seg_name = '/nafs/dtward/dong/upenn_atlas/UPenn_labels_reoriented_origin.vtk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87878c41-8dbc-4d0e-9cd9-5d77d4fcb28f",
   "metadata": {},
   "source": [
    "### Load UPenn ontology + Generate lists of descendents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xS,S,_,_ = emlddmm.read_data(seg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_column = 7 # 8 for allen, 7 for yongsoo\n",
    "label_column = 0 # 0 for both\n",
    "shortname_column = 2# 3 for allen, 2 for yongsoo\n",
    "longname_column = 1# 2 for allen, 1 for yongsoo\n",
    "ontology = dict()\n",
    "with open(ontology_name) as f:\n",
    "    csvreader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    count = 0\n",
    "    for row in csvreader:        \n",
    "        if count == 0:\n",
    "            headers = row\n",
    "            print(headers)\n",
    "        else:\n",
    "            if not row[parent_column]:\n",
    "                parent = -1\n",
    "            else:\n",
    "                parent = int(row[parent_column])\n",
    "            ontology[int(row[label_column])] = (row[shortname_column],row[longname_column],parent)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92746afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to find all the descendants of a given label\n",
    "# first we'll get children\n",
    "children = dict()\n",
    "for o in ontology:\n",
    "    parent = ontology[o][-1]\n",
    "    if parent not in children:\n",
    "        children[parent] = []\n",
    "    children[parent].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ecb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we go from children to descendents\n",
    "descendents = dict(children)\n",
    "for o in descendents:\n",
    "    for child in descendents[o]:\n",
    "        if child in descendents: # if I don't do this i get a key error 0\n",
    "            descendents[o].extend(descendents[child])\n",
    "descendents[0] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3beb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "descendents_and_self = dict(descendents)\n",
    "for o in ontology:\n",
    "    if o not in descendents_and_self:\n",
    "        descendents_and_self[o] = [o]\n",
    "    else:\n",
    "        descendents_and_self[o].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820bb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc42ac-3d87-4072-a59f-31bfbcbe8011",
   "metadata": {},
   "source": [
    "## Generate Boolean masks for CP, CPr (+CPre), CPi, CPc (+CPce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78115e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the descendents of Caudoputemen- rostral\n",
    "# and caudoputemen rostral- extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e69696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all the structures in the CP\n",
    "cp = list(descendents_and_self[672])\n",
    "# get caudate\n",
    "Scp = np.zeros_like(S)\n",
    "for l in cp:\n",
    "    Scp = np.logical_or(Scp,S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the structures one level below CP, with extreme's merged in\n",
    "rostral = list(descendents_and_self[2376]) # rostral extreme\n",
    "rostral.extend(list(descendents_and_self[2491])) # rostral \n",
    "rostral = list(dict.fromkeys(rostral))\n",
    "\n",
    "intermediate = list(descendents_and_self[2492]) # intermediate\n",
    "\n",
    "caudal = list(descendents_and_self[2496]) # caudal\n",
    "caudal_ = list(descendents_and_self[2495]) # caudal extreme\n",
    "caudal.extend(caudal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67210fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Srostral = np.zeros_like(S)\n",
    "for l in rostral:\n",
    "    Srostral = np.logical_or(Srostral, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sintermediate = np.zeros_like(S)\n",
    "for l in intermediate:\n",
    "    Sintermediate = np.logical_or(Sintermediate, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558600",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaudal = np.zeros_like(S)\n",
    "for l in caudal:\n",
    "    Scaudal = np.logical_or(Scaudal, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908afa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.sum(Srostral>0,axis=(0,2,3)),label='r')\n",
    "ax.plot(np.sum(Sintermediate>0,axis=(0,2,3)),label='i')\n",
    "ax.plot(np.sum(Scaudal>0,axis=(0,2,3)),label='c')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# add one more level down the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9184617",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2750940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think what I'd like to do is assign a gaussian to each region\n",
    "# then give the neurons a distribution based on the Gaussian\n",
    "# to do this I should load a set of neurons\n",
    "# and also start visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6016ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I think would make sense is to take all the cp structures\n",
    "# blur them a lot\n",
    "# then assign probabilities\n",
    "# we also want to look at the neurons though\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to start by visualizing\n",
    "# we need to load swc files\n",
    "# and we need to contour the surfaces\n",
    "d = [x[1] - x[0] for x in xS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "down = 16\n",
    "xSd,Scpd = emlddmm.downmode(xS,Scp[0],down=[down,down,down])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = [x[1] - x[0] for x in xSd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verts,faces,normals,values = marching_cubes(Scpd,level=0.5,spacing=dd)\n",
    "verts = verts + np.array([x[0] for x in xSd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623af01-26ff-4d4a-8906-9ad8f72069c2",
   "metadata": {},
   "source": [
    "## Load SWC files and Display 2x2 figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb54b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# swcdir = '../swc_out_v08' # this is tme07\n",
    "# swcdir = '../dragonfly_tme09-1/swc_out_v08'\n",
    "# swcdir = '/home/abenneck/dragonfly_work/dragonfly_outputs/TME08-1/dragonfly_joint_outputs'\n",
    "\n",
    "brain = 'TME12-1'\n",
    "swcdir = f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs'\n",
    "files = glob(join(swcdir,'*.swc'))\n",
    "files = [f for f in files if 'permuted' not in f]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856d357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for file in files:\n",
    "    with open(file)  as f:\n",
    "        for i,line in enumerate(f):    \n",
    "            print(line)\n",
    "            if 'Tward' in line:                \n",
    "                continue\n",
    "            else:\n",
    "                if ',' in line:\n",
    "                    lim = ','\n",
    "                else:\n",
    "                    lim = ''\n",
    "                coords = [float(c) for c in line.split(lim)[2:5]]                \n",
    "                    \n",
    "                x.append(coords)\n",
    "                break\n",
    "    \n",
    "            \n",
    "x = np.array(x)            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539eeab-c91e-4961-a246-f1c19789e1fd",
   "metadata": {},
   "source": [
    "### Generate + Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "alpha = 0.25\n",
    "lw = 0.25\n",
    "\n",
    "\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,2,1,projection='3d')\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,2,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(0,90)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,3,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(0,0)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,4,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(90,0)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "# fig.suptitle(swcdir.split('/')[-2])\n",
    "# fig.savefig('CP_SWC_QC_figure_'+swcdir.split('/')[-2]+'.jpg')\n",
    "\n",
    "fig.suptitle(f'{brain}')\n",
    "fig.savefig(join(f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/',f'CP_SWC_QC_figure_{brain}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450082b-f335-46d9-b784-f31d2785314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(f'End of QC figure generation for {brain}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2912c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so what is a plan going forward\n",
    "# I'd like to take every blob as a gaussian\n",
    "# the height is its volume\n",
    "# the mean is its mean\n",
    "# the covariance is its covariance\n",
    "# the only trouble here is the left right issue\n",
    "# another posiblility is to just blur the labels\n",
    "# this may get rid of small structures though\n",
    "# but I could give less blur to the small structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, xS[1] should be left right\n",
    "\n",
    "# Below line was ran in original version\n",
    "# xS[1]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can just make positive x1 to do this\n",
    "\n",
    "# Below lines were ran in original version\n",
    "# Srostral\n",
    "# Scaudal\n",
    "# Sintermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first going forward\n",
    "# 1. make a figure like this to get a sense of the uncertainty for each brain\n",
    "# 2. make a probabilitistic version of the CP structures.\n",
    "# I'd like to model each structure as a gaussian blob (ellispoids with soft boundaries)\n",
    "# this needs three parameters\n",
    "# the mean (a 3 element vector)\n",
    "# the covariance (3x3 symmetric matrix)\n",
    "# and the height/amplitude of the gaussian (one positive number)\n",
    "\n",
    "# The height (amplitude) should be the number of voxels in the structure\n",
    "# the mean, is going to be the first moment of the segmentation\n",
    "# the covariance, is the second central moment of the segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335669b-da7f-4643-a6e9-86df921dbebb",
   "metadata": {},
   "source": [
    "### Compute + Save Gaussian parameters for CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf38ac8-aafd-4683-95ce-b6cb31fbd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original Scp since variable may be updated in below cell\n",
    "Scp_ = Scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d68113",
   "metadata": {},
   "outputs": [],
   "source": [
    "updateFile = False\n",
    "# ===== Specify subregion and hemisphere =====\n",
    "\n",
    "# Scp = Scp_\n",
    "# Scp = Srostral\n",
    "# Scp = Sintermediate\n",
    "# Scp = Scaudal\n",
    "\n",
    "hemi = 'L'\n",
    "if hemi == 'L':\n",
    "    LR_indicator = (xS[1][:,None]>=0) # Left hemisphere?\n",
    "else:\n",
    "    LR_indicator = (xS[1][:,None]<0)  # Right hemisphere?\n",
    "\n",
    "# ===== Compute Gaussian parameters =====\n",
    "\n",
    "# number of voxels in CP\n",
    "Ncp = np.sum(Scp*LR_indicator)\n",
    "print(f'number of voxels {Ncp}')\n",
    "\n",
    "# note xS tells us the location of each voxel\n",
    "# let's compute the first moment\n",
    "mucp = [np.sum(xS[0][:,None,None]*Scp*LR_indicator)/Ncp, np.sum(xS[1][:,None]*Scp*LR_indicator)/Ncp, np.sum(xS[2][:]*Scp*LR_indicator)/Ncp]\n",
    "print(f'mu {mucp}')\n",
    "\n",
    "# now calculate the covariance matrix\n",
    "covcp01 = np.sum((xS[0][:,None,None] - mucp[0])*(xS[1][:,None] - mucp[1])*Scp*LR_indicator)/Ncp # here row 0 column 1\n",
    "covcp02 = np.sum((xS[0][:,None,None] - mucp[0])*(xS[2][None,:] - mucp[2])*Scp*LR_indicator)/Ncp # here row 0 column 2\n",
    "covcp12 = np.sum((xS[1][None,:,None] - mucp[1])*(xS[2][None,:] - mucp[2])*Scp*LR_indicator)/Ncp # here row 1 column 2\n",
    "\n",
    "covcp00 = np.sum(((xS[0][:,None,None] - mucp[0])**2)*Scp*LR_indicator)/Ncp # here row 0 column 0\n",
    "covcp11 = np.sum(((xS[1][None,:,None] - mucp[1])**2)*Scp*LR_indicator)/Ncp # here row 1 column 1\n",
    "covcp22 = np.sum(((xS[2][None,None,:] - mucp[2])**2)*Scp*LR_indicator)/Ncp # here row 2 column 2\n",
    "\n",
    "covcp = [[covcp00,covcp01,covcp02],[covcp01,covcp11,covcp12],[covcp02,covcp12,covcp22]]\n",
    "\n",
    "print('cov:')\n",
    "print(f'{covcp[0]}')\n",
    "print(f'{covcp[1]}')\n",
    "print(f'{covcp[2]}')\n",
    "\n",
    "# Save Gaussian parameters in.npz file with keys [height, mu, cov]\n",
    "if updateFile:\n",
    "    fname = f'cpc_param_{hemi}.npz'\n",
    "    save_path = join('/home/abenneck/dragonfly_work/gaussian_parameters/',fname)\n",
    "    np.savez(save_path,height=[Ncp],mu=mucp,cov=covcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e9fd0-bade-4cb6-9437-b782271d82e4",
   "metadata": {},
   "source": [
    "### Load parameters given hemi and subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959276e-6650-47a0-9930-d6cc08ea2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = 'L'\n",
    "subregion = ''\n",
    "\n",
    "fname = f'cp{subregion}_param_{hemi}.npz'\n",
    "fpath = join(f'/home/abenneck/dragonfly_work/gaussian_parameters/',fname)\n",
    "out = np.load(fpath)\n",
    "\n",
    "h = out['height'][0]\n",
    "mu = out['mu']\n",
    "cov = out['cov']\n",
    "\n",
    "print(f'Displating paremeters for {fname};\\n')\n",
    "print(f'height: {h}')\n",
    "print(f'mu: {mu}')\n",
    "print(f'cov: {cov}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have parameters for each gaussian\n",
    "# then we can evalute them at every point in space\n",
    "# so for a given cell body, we can find a distribution over this number of regions\n",
    "# for each level of the tree, you can get a distribution over all the structures in that level\n",
    "# start with R-I-C level\n",
    "# TODO, come up with some measure of concordance\n",
    "#   that is related to nick's labels, but allows some slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't really believe the uncertaint is given by the voxel size (we're not getting 1 voxel accurate registration)\n",
    "# we believe it is related to the anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cdf9c-f49f-4caa-87ee-b1b8184e865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import os\n",
    "\n",
    "# Define relevant directories\n",
    "brain = 'TME08-1'\n",
    "neuronDir = f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs/'\n",
    "paramDir = '/home/abenneck/dragonfly_work/gaussian_parameters'\n",
    "allDist = list()\n",
    "allProb = pd.DataFrame(columns=['name','CP_L','CP_R','CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R'])\n",
    "\n",
    "# Create list of multivariate Gaussian RVs from presaved parameters\n",
    "for file in sorted(os.listdir(paramDir)[1:]):\n",
    "    data = np.load(os.path.join(paramDir,file))\n",
    "    norm_factor = data['height'].item()\n",
    "    mu = data['mu']\n",
    "    cov = data['cov']\n",
    "    allDist.append([multivariate_normal(mean=mu, cov=cov, allow_singular=False),norm_factor])\n",
    "\n",
    "# Generate regional probabilities for every neuron in neuronDir\n",
    "row_idx = 0\n",
    "for file in sorted(os.listdir(neuronDir)):\n",
    "    if \"permuted.swc\" in file:\n",
    "        # Load soma coordinates\n",
    "        data = pd.read_csv(os.path.join(neuronDir,file))\n",
    "        x = data.columns[2]\n",
    "        y = data.columns[3]\n",
    "        z = data.columns[4]\n",
    "        soma_location = [x,y,z]\n",
    "\n",
    "        # Compute probabilities form multinomial Gaussians\n",
    "        neuronProb = [file]\n",
    "        for dist in allDist:\n",
    "            neuronProb.append(dist[0].pdf(soma_location)*dist[1])\n",
    "\n",
    "        # Normalize probabilities at different scales\n",
    "        neuronProb[1:3] = neuronProb[1:3] / np.sum(neuronProb[1:3]) # CP (L+R)\n",
    "        neuronProb[3:] = neuronProb[3:] / np.sum(neuronProb[3:])    # CPc, CPi, CPr (L+R)\n",
    "        neuronProb[1:] = [f'{x:.3e}' for x in neuronProb[1:]]       # 4 sig figs\n",
    "        \n",
    "        # neuronProb.insert(0,file)\n",
    "        allProb.loc[row_idx] = neuronProb\n",
    "        row_idx+=1\n",
    "\n",
    "tempDir = f'/home/abenneck/dragonfly_work/{brain}_neuron_region_prob.csv'\n",
    "\n",
    "allProb.to_csv(tempDir, index=False)\n",
    "\n",
    "# allProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2874d-1086-49f4-88c5-160275920345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959a3b6-5298-417e-805e-6efe22e34eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0.000000000000123456789\n",
    "\n",
    "print(f'{val:.3e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
