{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13e325",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/dtward/data/csh_data/emlddmm')\n",
    "sys.path.append('/home/abenneck/Desktop/emlddmm')\n",
    "import emlddmm\n",
    "import csv\n",
    "from skimage.measure import marching_cubes\n",
    "from glob import glob\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from os.path import split,join,splitext\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import os\n",
    "\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import time\n",
    "\n",
    "sys.path.append('/home/abenneck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0974bc-c61c-4590-b3a9-9aa02d09077a",
   "metadata": {},
   "source": [
    "# 0. Define region IDs and file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5c106",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ontology_name = '/home/abenneck/nafs/dtward/dong/upenn_atlas/atlas_info_KimRef_FPbasedLabel_v2.7.csv'\n",
    "seg_name = '/home/abenneck/nafs/dtward/dong/upenn_atlas/UPenn_labels_reoriented_origin.vtk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69426dc-e688-4216-a7f1-3d80465d4be0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# L0 CP regions\n",
    "regionID_cp = 672 # Caudate Putamen\n",
    "\n",
    "# L1 CP regions\n",
    "regionID_cpr = 2491     # CP, rostral\n",
    "regionID_cpre = 2376    # CP, rostral extreme\n",
    "regionID_cpi = 2492     # CP, intermediate\n",
    "regionID_cpc = 2496     # CP, caudal\n",
    "regionID_cpce = 2495    # CP, caudal extreme\n",
    "\n",
    "# L2 CP regions\n",
    "regionID_cpr_m = 2294   # CPr, medial\n",
    "regionID_cpr_imd = 2295 # CPr, intermediate dorsal\n",
    "regionID_cpr_imv = 2296 # CPr, intermediate ventral\n",
    "regionID_cpr_l = 2497   # CPr, lateral\n",
    "regionID_cpi_dm = 2498  # CPi, dorsomedial\n",
    "regionID_cpi_vm = 2500  # CPi, ventromedial\n",
    "regionID_cpi_dl = 2499  # CPi, dorsolateral\n",
    "regionID_cpi_vl = 2501  # CPi, ventrolateral\n",
    "regionID_cpc_d = 2493   # CPc, dorsal\n",
    "regionID_cpc_i = 2494   # CPc, intermediate\n",
    "regionID_cpc_v = 2490   # CPc, ventral\n",
    "\n",
    "# Non-CP gray matter regions\n",
    "regionID_ast = 2050     # Amygdalostriatal nucleus\n",
    "regionID_acb = 56       # Accumbens nucleus\n",
    "regionID_ipac = 998     # Interstitial nucleus of the posterior limb of the anterior commissure\n",
    "regionID_tu = 754       # Olfactory tubercle\n",
    "regionID_lsx = 275      # Lateral septal complex\n",
    "regionID_samy = 278     # Striatum-like amygdalar nuclei\n",
    "regionID_pal = 803      # Pallidum\n",
    "regionID_cl = 583       # Claustrum\n",
    "regionID_en = 942       # Endopiriform nucleus\n",
    "regionID_pir = 961      # Piriform nucleus\n",
    "\n",
    "# Non-CP regions\n",
    "regionID_lv = 81        # Lateral ventricle\n",
    "regionID_3V = 129       # 3rd ventricle\n",
    "regionID_Aq = 140       # Aqueduct\n",
    "regionID_4V = 145       # 4th ventricle\n",
    "regionID_centCan = 164  # Central canal\n",
    "regionID_cc = 776       # Corpus callosum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87878c41-8dbc-4d0e-9cd9-5d77d4fcb28f",
   "metadata": {},
   "source": [
    "### Load UPenn ontology + Generate lists of descendents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xS,S,_,_ = emlddmm.read_data(seg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad692ab1-c529-47e9-a07b-911a97413197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms xS into a set of coordinates\n",
    "XS = np.stack(np.meshgrid(*xS,indexing='ij'),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba5cc1-f8ac-44ac-ae72-eff190026cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center of mass of region 2294 (Definition of the 1st moment of the indicator function for label 2294)\n",
    "# np.sum(XS*(S[0,...,None]==2294))/np.sum(S==2294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_column = 7 # 8 for allen, 7 for yongsoo\n",
    "label_column = 0 # 0 for both\n",
    "shortname_column = 2# 3 for allen, 2 for yongsoo\n",
    "longname_column = 1# 2 for allen, 1 for yongsoo\n",
    "ontology = dict()\n",
    "with open(ontology_name) as f:\n",
    "    csvreader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    count = 0\n",
    "    for row in csvreader:        \n",
    "        if count == 0:\n",
    "            headers = row\n",
    "            print(headers)\n",
    "        else:\n",
    "            if not row[parent_column]:\n",
    "                parent = -1\n",
    "            else:\n",
    "                parent = int(row[parent_column])\n",
    "            ontology[int(row[label_column])] = (row[shortname_column],row[longname_column],parent)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92746afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to find all the descendants of a given label\n",
    "# first we'll get children\n",
    "children = dict()\n",
    "for o in ontology:\n",
    "    parent = ontology[o][-1]\n",
    "    if parent not in children:\n",
    "        children[parent] = []\n",
    "    children[parent].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ecb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we go from children to descendents\n",
    "descendents = dict(children)\n",
    "for o in descendents:\n",
    "    for child in descendents[o]:\n",
    "        if child in descendents: # if I don't do this i get a key error 0\n",
    "            descendents[o].extend(descendents[child])\n",
    "descendents[0] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3beb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "descendents_and_self = dict(descendents)\n",
    "for o in ontology:\n",
    "    if o not in descendents_and_self:\n",
    "        descendents_and_self[o] = [o]\n",
    "    else:\n",
    "        descendents_and_self[o].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820bb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc42ac-3d87-4072-a59f-31bfbcbe8011",
   "metadata": {},
   "source": [
    "## Generate Boolean masks for CP, CPr (+CPre), CPi, CPc (+CPce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78115e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the descendents of Caudoputemen- rostral\n",
    "# and caudoputemen rostral- extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e69696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all the structures in the CP\n",
    "cp = list(descendents_and_self[672])\n",
    "Scp = np.zeros_like(S)\n",
    "for l in cp:\n",
    "    Scp = np.logical_or(Scp,S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the structures one level below CP, with extreme's merged in\n",
    "rostral = list(descendents_and_self[2376]) # rostral extreme\n",
    "rostral.extend(list(descendents_and_self[2491])) # rostral \n",
    "rostral = list(dict.fromkeys(rostral))\n",
    "\n",
    "intermediate = list(descendents_and_self[2492]) # intermediate\n",
    "\n",
    "caudal = list(descendents_and_self[2496]) # caudal\n",
    "caudal_ = list(descendents_and_self[2495]) # caudal extreme\n",
    "caudal.extend(caudal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67210fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Srostral = np.zeros_like(S)\n",
    "for l in rostral:\n",
    "    Srostral = np.logical_or(Srostral, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sintermediate = np.zeros_like(S)\n",
    "for l in intermediate:\n",
    "    Sintermediate = np.logical_or(Sintermediate, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558600",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaudal = np.zeros_like(S)\n",
    "for l in caudal:\n",
    "    Scaudal = np.logical_or(Scaudal, S==l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908afa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_x, atlas_y, atlas_z = xS\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.sum(Srostral>0,axis=(0,2,3)),label='r')\n",
    "ax.plot(np.sum(Sintermediate>0,axis=(0,2,3)),label='i')\n",
    "ax.plot(np.sum(Scaudal>0,axis=(0,2,3)),label='c')\n",
    "# ax.set_xticks(atlas_x, minor=True)\n",
    "# ax.minorticks_off()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cd6bb-4d29-4702-8410-3f0c02edbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_bound = 370*(atlas_x[1]-atlas_x[0])+np.min(atlas_x)\n",
    "ir_bound = 423*(atlas_x[1]-atlas_x[0])+np.min(atlas_x)\n",
    "\n",
    "print(f'ci: {ci_bound}, ir: {ir_bound}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_region_mask(S, regionID):\n",
    "    self_and_children = list(descendents_and_self[regionID])\n",
    "    region_mask = np.zeros_like(S)\n",
    "    for i in self_and_children:\n",
    "        region_mask = np.logical_or(region_mask, S==i)\n",
    "    return region_mask\n",
    "\n",
    "# === Striatum Dorsal Region (STRd) ===\n",
    "\n",
    "# CP regions (L2)\n",
    "Scpr_m   = generate_region_mask(S,2294)\n",
    "Scpr_imd = generate_region_mask(S,2295)\n",
    "Scpr_imv = generate_region_mask(S,2296)\n",
    "Scpr_l   = generate_region_mask(S,2497)\n",
    "Scpi_dm  = generate_region_mask(S,2498)\n",
    "Scpi_vm  = generate_region_mask(S,2500)\n",
    "Scpi_dl  = generate_region_mask(S,2499)\n",
    "Scpi_vl  = generate_region_mask(S,2501)\n",
    "Scpc_d   = generate_region_mask(S,2493)\n",
    "Scpc_i   = generate_region_mask(S,2494)\n",
    "Scpc_v   = generate_region_mask(S,2490)\n",
    "\n",
    "# Non-CP regions\n",
    "# S_lss = generate_region_mask(S,2001) # Lateral strip of the striatum (Outdated)\n",
    "S_ast = generate_region_mask(S,2050) # Amygdalostriatial transition\n",
    "\n",
    "# === Striatum Ventral Region (STRv) ===\n",
    "S_acb  = generate_region_mask(S,56)  # Accumbens nucleus\n",
    "S_ipac = generate_region_mask(S,998) # Interstitial nucleus of the posterior limb of the anterior commissure\n",
    "S_tu   = generate_region_mask(S,754) # Olfactory tubercle\n",
    "\n",
    "# === (10/3/23) Striatum adjacent regions ===\n",
    "S_lsx = generate_region_mask(S,275)  # Lateral septal complex\n",
    "S_samy = generate_region_mask(S,278) # Striatum-like amygdalar nuclei\n",
    "S_pal = generate_region_mask(S,803)  # Pallidum\n",
    "\n",
    "# === (10/4/23) Striatum adjacent regions ===\n",
    "S_cl =   generate_region_mask(S,583) # Claustrum\n",
    "S_en =   generate_region_mask(S,942) # Endopiriform nucleus\n",
    "S_pir =  generate_region_mask(S,961) # Piriform nucleus\n",
    "\n",
    "# === (11/6/23) Regions for distance computations ===\n",
    "S_lv = generate_region_mask(S,regionID_lv)\n",
    "S_3V = generate_region_mask(S,regionID_3V)\n",
    "S_Aq = generate_region_mask(S,regionID_Aq)\n",
    "S_4V = generate_region_mask(S,regionID_4V)\n",
    "S_centralCanal = generate_region_mask(S,regionID_centCan)\n",
    "\n",
    "S_cc = generate_region_mask(S,regionID_cc)\n",
    "\n",
    "S_cp_and_acb = np.logical_or(Scp, S_acb == 1)   # Create mask for all CP and ACB voxels\n",
    "S_not_cp_or_acb = np.zeros_like(S)\n",
    "S_not_cp_or_acb[S != 0] = 1                     # Create a copy of S where all nonzero voxels are 1\n",
    "S_not_cp_or_acb[S_cp_and_acb == 1] = 0          # Set all voxels witin CP or ACB to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c38981-9fd6-44aa-b024-07e49e5d565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid = np.zeros_like(S)\n",
    "# mid[S!=0] = 1\n",
    "\n",
    "# print(f'{np.sum(S_cp_and_acb):,} + {np.sum(S_not_cp_or_acb):,} = {np.sum(S_cp_and_acb) + np.sum(S_not_cp_or_acb):,} ({np.sum(mid):,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e8fe7-7b75-41bb-8548-f42602ab6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Returns a mask where boundary voxels are 1 and all other voxels are 0\n",
    "# def generate_brain_boundary_mask(S):\n",
    "#     boundary_mask = distance_transform_edt(S)  # Computes distance transform of entire brain\n",
    "#     boundary_mask[boundary_mask != 1.0] = 0.0  # Only boundary pixels have a value of 1.0 (pixels inside > 1.0 and pixels outside = 0.0)\n",
    "#     return boundary_mask\n",
    "\n",
    "# S_brain_bound = generate_brain_boundary_mask(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2750940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think what I'd like to do is assign a gaussian to each region\n",
    "# then give the neurons a distribution based on the Gaussian\n",
    "# to do this I should load a set of neurons\n",
    "# and also start visualizing\n",
    "\n",
    "# what I think would make sense is to take all the cp structures\n",
    "# blur them a lot\n",
    "# then assign probabilities\n",
    "# we also want to look at the neurons though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to start by visualizing\n",
    "# we need to load swc files\n",
    "# and we need to contour the surfaces\n",
    "d = [x[1] - x[0] for x in xS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "down = 16\n",
    "xSd,Scpd = emlddmm.downmode(xS,Scp[0],down=[down,down,down])\n",
    "\n",
    "dd = [x[1] - x[0] for x in xSd]\n",
    "\n",
    "verts,faces,normals,values = marching_cubes(Scpd,level=0.5,spacing=dd)\n",
    "verts = verts + np.array([x[0] for x in xSd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623af01-26ff-4d4a-8906-9ad8f72069c2",
   "metadata": {},
   "source": [
    "## Load SWC files and Display 2x2 figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb54b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# swcdir = '../swc_out_v08' # this is tme07\n",
    "# swcdir = '../dragonfly_tme09-1/swc_out_v08'\n",
    "# swcdir = '/home/abenneck/dragonfly_work/dragonfly_outputs/TME08-1/dragonfly_joint_outputs'\n",
    "\n",
    "brain = 'TME08-1'\n",
    "swcdir = f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs'\n",
    "files = glob(join(swcdir,'*.swc'))\n",
    "files = [f for f in files if 'permuted' not in f]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856d357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for file in files:\n",
    "    with open(file)  as f:\n",
    "        for i,line in enumerate(f):    \n",
    "            print(line)\n",
    "            if 'Tward' in line:                \n",
    "                continue\n",
    "            else:\n",
    "                if ',' in line:\n",
    "                    lim = ','\n",
    "                else:\n",
    "                    lim = ''\n",
    "                coords = [float(c) for c in line.split(lim)[2:5]]                \n",
    "                    \n",
    "                x.append(coords)\n",
    "                break\n",
    "    \n",
    "            \n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539eeab-c91e-4961-a246-f1c19789e1fd",
   "metadata": {},
   "source": [
    "### Generate + Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "alpha = 0.25\n",
    "lw = 0.25\n",
    "\n",
    "\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,2,1,projection='3d')\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,2,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(0,90)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,3,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(0,0)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2,2,4,projection='3d')\n",
    "mesh = Poly3DCollection(verts[faces],ec=[0.0,0.0,0.0,0.1],lw=lw,alpha=alpha,)\n",
    "ax.view_init(90,0)\n",
    "ax.add_collection3d(mesh)\n",
    "# set limits uniform\n",
    "vertsmin = np.min(verts,0)\n",
    "vertsmax = np.max(verts,0)\n",
    "vertsc = vertsmin*0.5 + vertsmax*0.5\n",
    "vertsd = vertsmax-vertsmin\n",
    "vertsd = np.max(vertsd)\n",
    "lim = vertsc[None] + np.array([-1,1])[...,None]/2*vertsd\n",
    "ax.set_xlim(lim[:,0])\n",
    "ax.set_ylim(lim[:,1])\n",
    "ax.set_zlim(lim[:,2])\n",
    "ax.set_xlabel('x0')\n",
    "ax.set_ylabel('x1')\n",
    "ax.set_zlabel('x2')\n",
    "ax.scatter(x[:,0],x[:,1],x[:,2],s=s)\n",
    "\n",
    "# fig.suptitle(swcdir.split('/')[-2])\n",
    "# fig.savefig('CP_SWC_QC_figure_'+swcdir.split('/')[-2]+'.jpg')\n",
    "\n",
    "fig.suptitle(f'{brain}')\n",
    "# fig.savefig(join(f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/',f'CP_SWC_QC_figure_{brain}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450082b-f335-46d9-b784-f31d2785314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(f'End of QC figure generation for {brain}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d73dd-7a61-45d7-8b36-909409898f96",
   "metadata": {},
   "source": [
    "# 1. Probability Regional Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2912c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so what is a plan going forward\n",
    "# I'd like to take every blob as a gaussian\n",
    "# the height is its volume\n",
    "# the mean is its mean\n",
    "# the covariance is its covariance\n",
    "# the only trouble here is the left right issue\n",
    "# another posiblility is to just blur the labels\n",
    "# this may get rid of small structures though\n",
    "# but I could give less blur to the small structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first going forward\n",
    "# 1. make a figure like this to get a sense of the uncertainty for each brain\n",
    "# 2. make a probabilitistic version of the CP structures.\n",
    "# I'd like to model each structure as a gaussian blob (ellispoids with soft boundaries)\n",
    "# this needs three parameters\n",
    "# the mean (a 3 element vector)\n",
    "# the covariance (3x3 symmetric matrix)\n",
    "# and the height/amplitude of the gaussian (one positive number)\n",
    "\n",
    "# The height (amplitude) should be the number of voxels in the structure\n",
    "# the mean, is going to be the first moment of the segmentation\n",
    "# the covariance, is the second central moment of the segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335669b-da7f-4643-a6e9-86df921dbebb",
   "metadata": {},
   "source": [
    "### Compute + Save Gaussian parameters for CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d68113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_gaussian_param(subregion_mask, hemi, fname):\n",
    "    if hemi == 'R':\n",
    "        LR_indicator = (xS[1][:,None]>=0) # Right hemisphere\n",
    "    else:\n",
    "        LR_indicator = (xS[1][:,None]<0)  # Left hemisphere\n",
    "    \n",
    "    # ===== Compute Gaussian parameters =====\n",
    "    \n",
    "    # number of voxels in 'subregion'\n",
    "    Ncp = np.sum(subregion_mask*LR_indicator)\n",
    "    # print(f'number of voxels {Ncp}')\n",
    "    \n",
    "    # note xS tells us the location of each voxel\n",
    "    # let's compute the first moment\n",
    "    mucp = [np.sum(xS[0][:,None,None]*subregion_mask*LR_indicator)/Ncp, np.sum(xS[1][:,None]*subregion_mask*LR_indicator)/Ncp, np.sum(xS[2][:]*subregion_mask*LR_indicator)/Ncp]\n",
    "    # print(f'mu {mucp}')\n",
    "    \n",
    "    # now calculate the covariance matrix\n",
    "    covcp01 = np.sum((xS[0][:,None,None] - mucp[0])*(xS[1][:,None] - mucp[1])*subregion_mask*LR_indicator)/Ncp # here row 0 column 1\n",
    "    covcp02 = np.sum((xS[0][:,None,None] - mucp[0])*(xS[2][None,:] - mucp[2])*subregion_mask*LR_indicator)/Ncp # here row 0 column 2\n",
    "    covcp12 = np.sum((xS[1][None,:,None] - mucp[1])*(xS[2][None,:] - mucp[2])*subregion_mask*LR_indicator)/Ncp # here row 1 column 2\n",
    "    \n",
    "    covcp00 = np.sum(((xS[0][:,None,None] - mucp[0])**2)*subregion_mask*LR_indicator)/Ncp # here row 0 column 0\n",
    "    covcp11 = np.sum(((xS[1][None,:,None] - mucp[1])**2)*subregion_mask*LR_indicator)/Ncp # here row 1 column 1\n",
    "    covcp22 = np.sum(((xS[2][None,None,:] - mucp[2])**2)*subregion_mask*LR_indicator)/Ncp # here row 2 column 2\n",
    "    \n",
    "    covcp = [[covcp00,covcp01,covcp02],[covcp01,covcp11,covcp12],[covcp02,covcp12,covcp22]]\n",
    "    \n",
    "    # print('cov:')\n",
    "    # print(f'{covcp[0]}')\n",
    "    # print(f'{covcp[1]}')\n",
    "    # print(f'{covcp[2]}')\n",
    "    \n",
    "    # Save Gaussian parameters in.npz file with keys [height, mu, cov]\n",
    "    save_path = join('/home/abenneck/dragonfly_work/gaussian_parameters/',fname)\n",
    "    np.savez(save_path,height=[Ncp],mu=mucp,cov=covcp)\n",
    "    print(f'Saved {fname}\\n')\n",
    "\n",
    "genParam = False\n",
    "if genParam:\n",
    "    for hemi in ['L','R']:\n",
    "        generate_gaussian_param(Scp, hemi,f'cp_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Srostral, hemi,f'cpr_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Sintermediate, hemi,f'cpi_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scaudal, hemi,f'cpc_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(Scpr_m, hemi,f'cpr_m_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpr_imd, hemi,f'cpr_imd_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpr_imv, hemi,f'cpr_imv_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpr_l, hemi,f'cpr_l_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(Scpi_dm, hemi,f'cpi_dm_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpi_vm, hemi,f'cpi_vm_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpi_dl, hemi,f'cpi_dl_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpi_vl, hemi,f'cpi_vl_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(Scpc_d, hemi,f'cpc_d_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpc_i, hemi,f'cpc_i_param_{hemi}.npz')\n",
    "        generate_gaussian_param(Scpc_v, hemi,f'cpc_v_param_{hemi}.npz')\n",
    "\n",
    "        generate_gaussian_param(S_ast, hemi, f'ast_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(S_acb, hemi, f'acb_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_ipac, hemi, f'ipac_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_tu, hemi, f'tu_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(S_lsx, hemi, f'lsx_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_samy, hemi, f'samy_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_pal, hemi, f'pal_param_{hemi}.npz')\n",
    "        \n",
    "        generate_gaussian_param(S_cl, hemi,f'cl_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_en, hemi,f'en_param_{hemi}.npz')\n",
    "        generate_gaussian_param(S_pir, hemi,f'pir_param_{hemi}.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e9fd0-bade-4cb6-9437-b782271d82e4",
   "metadata": {},
   "source": [
    "### Load Gaussian parameters given hemi and subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959276e-6650-47a0-9930-d6cc08ea2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hemi = 'L'\n",
    "# hemi = 'R'\n",
    "# subregion = ''\n",
    "\n",
    "# fname = f'cpc_v_param_{hemi}.npz'\n",
    "# fpath = join(f'/home/abenneck/dragonfly_work/gaussian_parameters/',fname)\n",
    "# out = np.load(fpath)\n",
    "\n",
    "# h = out['height'][0]\n",
    "# mu = out['mu']\n",
    "# cov = out['cov']\n",
    "\n",
    "# print(f'Displating parameters for {fname};\\n')\n",
    "# print(f'height: {h}')\n",
    "# print(f'mu: {mu}')\n",
    "# print(f'cov: {cov}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have parameters for each gaussian\n",
    "# then we can evalute them at every point in space\n",
    "# so for a given cell body, we can find a distribution over this number of regions\n",
    "# for each level of the tree, you can get a distribution over all the structures in that level\n",
    "# start with R-I-C level\n",
    "# TODO, come up with some measure of concordance\n",
    "#   that is related to nick's labels, but allows some slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't really believe the uncertaint is given by the voxel size (we're not getting 1 voxel accurate registration)\n",
    "# we believe it is related to the anatomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd954b7-a66c-4ba6-a893-99f779adf4f1",
   "metadata": {},
   "source": [
    "### Compute distance transform for each mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c1eea-0b78-4bbc-a3d4-9d16b1b52b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateDistanceTransform(xS, mask, hemi, fname, save_path = '', scale_factor = 10):\n",
    "    start = time.time()\n",
    "    if hemi == 'R':\n",
    "        LR_indicator = (xS[1][:,None]>=0) # Right hemisphere\n",
    "    elif hemi == 'L':\n",
    "        LR_indicator = (xS[1][:,None]<0)  # Left hemisphere\n",
    "    else:\n",
    "        LR_indicator = 1.0 # Whole brain\n",
    "\n",
    "    if save_path == '':\n",
    "        save_path = '/home/abenneck/dragonfly_work/distance_parameters/'\n",
    "\n",
    "    transformedMask = distance_transform_edt(1-mask*LR_indicator)\n",
    "      \n",
    "    # Save distance transform parameters in .npz file with keys [mask]\n",
    "    save_path = join(save_path,fname)\n",
    "    np.savez(save_path, mask = transformedMask.astype(np.float32))\n",
    "    print(f'Saved {fname} after {time.time() - start : .2f} s')\n",
    "    \n",
    "    return transformedMask\n",
    "\n",
    "saveDistanceParam = False\n",
    "if saveDistanceParam:\n",
    "    for hemi in ['L','R']:\n",
    "        generateDistanceTransform(xS, Scp, hemi, f'cp_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Srostral, hemi, f'cpr_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Sintermediate, hemi, f'cpi_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scaudal, hemi, f'cpc_dist_param_{hemi}.npz')\n",
    "    \n",
    "        generateDistanceTransform(xS, Scpr_m, hemi, f'cpr_m_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpr_imd, hemi, f'cpr_imd_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpr_imv, hemi, f'cpr_imv_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpr_l, hemi, f'cpr_l_dist_param_{hemi}.npz')\n",
    "    \n",
    "        generateDistanceTransform(xS, Scpi_dm, hemi, f'cpi_dm_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpi_vm, hemi, f'cpi_vm_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpi_dl, hemi, f'cpi_dl_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpi_vl, hemi, f'cpi_vl_dist_param_{hemi}.npz')\n",
    "    \n",
    "        generateDistanceTransform(xS, Scpc_d, hemi, f'cpc_d_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpc_i, hemi, f'cpc_i_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, Scpc_v, hemi, f'cpc_v_dist_param_{hemi}.npz')\n",
    "\n",
    "        generateDistanceTransform(xS, S_ast, hemi, f'ast_dist_param_{hemi}.npz')\n",
    "        \n",
    "        generateDistanceTransform(xS, S_acb, hemi, f'acb_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_ipac, hemi, f'ipac_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_tu, hemi, f'tu_dist_param_{hemi}.npz')\n",
    "        \n",
    "        generateDistanceTransform(xS, S_lsx, hemi, f'lsx_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_samy, hemi, f'samy_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_pal, hemi, f'pal_dist_param_{hemi}.npz')\n",
    "\n",
    "        generateDistanceTransform(xS, S_cl, hemi, f'cl_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_en, hemi, f'en_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_pir, hemi, f'pir_dist_param_{hemi}.npz')\n",
    "\n",
    "        generateDistanceTransform(xS, S_lv, hemi, f'lv_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_3V, hemi, f'3V_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_Aq, hemi, f'Aq_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_4V, hemi, f'4V_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_centralCanal, hemi, f'centralCanal_dist_param_{hemi}.npz')\n",
    "        generateDistanceTransform(xS, S_cc, hemi, f'cc_dist_param_{hemi}.npz')\n",
    "\n",
    "    generateDistanceTransform(xS, S_cp_and_acb, 'w', f'cp_acb_dist_param.npz')\n",
    "    generateDistanceTransform(xS, S_not_cp_or_acb, 'w', f'not_cp_acb_dist_param.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb16b5-3732-4bcd-8cba-c7ba5af54908",
   "metadata": {},
   "source": [
    "### Load dist param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06ee5b-9e47-4aaa-8550-49c992cd01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = np.load('/home/abenneck/dragonfly_work/distance_parameters/cp_dist_param_L.npz')\n",
    "# out = out['mask']\n",
    "\n",
    "# x, y, z = 100, 100, -100\n",
    "\n",
    "# print(out[0, x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dbfa9-d389-4c3b-b307-862cde211804",
   "metadata": {},
   "source": [
    "### Preprocessing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0541ee-04db-4f72-9f78-1771b57d1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter which returns true for all distance transforms for L1 CP structures\n",
    "def L1_filter_dist(fileName):\n",
    "    if len(fileName) <= 21 and fileName != '.ipynb_checkpoints' and 'cp' in fileName and wm_vent_filter(fileName) and multi_filter(fileName):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Filter which returns true for all distance transforms for L2 CP structures\n",
    "def L2_filter_dist(fileName):\n",
    "    if len(fileName) > 21 and fileName != '.ipynb_checkpoints' and 'cp' in fileName and wm_vent_filter(fileName) and multi_filter(fileName):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "# Filter which returns true for all distance transforms for all non-CP gray matter structures\n",
    "def non_CP_filter_dist(fileName):\n",
    "    if not L2_filter_dist(fileName) and not L1_filter_dist(fileName) and fileName != '.ipynb_checkpoints' and wm_vent_filter(fileName) and multi_filter(fileName):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Filter which returns true for all distance transforms that are NOT white matter or ventricle structures\n",
    "def wm_vent_filter(fileName):\n",
    "    fnames_to_ignore = ['lv_dist_param_L.npz','ivf_dist_param_L.npz', '3V_dist_param_L.npz', 'Aq_dist_param_L.npz', '4V_dist_param_L.npz', 'centralCanal_dist_param_L.npz', 'cc_dist_param_L.npz', 'lv_dist_param_R.npz', 'ivf_dist_param_R.npz', '3V_dist_param_R.npz', 'Aq_dist_param_R.npz', '4V_dist_param_R.npz', 'centralCanal_dist_param_R.npz', 'cc_dist_param_R.npz']\n",
    "    for fname in fnames_to_ignore:\n",
    "        if fname == fileName:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Filter to skip cp_acb and not_cp_acb\n",
    "def multi_filter(fileName):\n",
    "    fnames_to_ignore = ['cp_acb_dist_param.npz','not_cp_acb_dist_param.npz']\n",
    "    for fname in fnames_to_ignore:\n",
    "        if fname == fileName:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Returns list of (x,y,z) coordinates for 'region_id' and any child subregions\n",
    "def get_coord_list(region_id, XS, S):\n",
    "    self_and_children = list(descendents_and_self[region_id])\n",
    "    allCoords = list()\n",
    "    for idNum in self_and_children:\n",
    "        regionCoords = XS[S[0]==idNum]\n",
    "        if len(regionCoords) != 0:\n",
    "            for coord in regionCoords:\n",
    "                allCoords.append(coord)\n",
    "    return allCoords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018300b3-6238-4a0b-a80a-796c773ec5e9",
   "metadata": {},
   "source": [
    "### Probability computation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b2cf2-b6fb-4ded-bd1a-5c3f4d07c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes minimum Euclidean distance between 'soma_location' ([x, y, z]) and 'regionCoords' (n * [x, y, z])\n",
    "def dist_from_region(regionCoords, soma_location, useForLoop = False):\n",
    "    if useForLoop:\n",
    "        minDist = -1\n",
    "        for c in allCoords:\n",
    "            distance = math.dist(c,soma_location)\n",
    "            if minDist == -1 or minDist > distance:\n",
    "                minDist = distance\n",
    "        return minDist        \n",
    "    else:\n",
    "        # out = allCoords-soma_location => [allCoords[0]-soma_location, allCoords[1]-soma_location, ...]\n",
    "        # out = (out)**2                => Square all terms within out\n",
    "        # out = np.sum(out, axis=1)     => [out[0][0]+out[0][1]+out[0][2], out[1][0]+out[1][1]+out[1][2], ...]\n",
    "        # out = np.sqrt(out)            => sqrt all terms within out\n",
    "        # out = np.min(out)             => min of all distances\n",
    "        return np.min(np.sqrt(np.sum((regionCoords-soma_location)**2, axis=1)))\n",
    "\n",
    "# Round neuron coord to nearest atlas coord\n",
    "def roundCoord(sub_xS, coord):\n",
    "    if coord < np.min(sub_xS):\n",
    "        coord = np.min(sub_xS)\n",
    "    elif coord > np.max(sub_xS):\n",
    "        coord = np.max(sub_xS)\n",
    "    else:\n",
    "        coord_rounded = 20*round(coord/20) # Round coordinate to nearest 20\n",
    "        if coord < 0:\n",
    "            if coord - coord_rounded >= 0:\n",
    "                coord = coord_rounded + 10\n",
    "            else:\n",
    "                coord = coord_rounded - 10\n",
    "        else:\n",
    "            if coord - coord_rounded >= 0:\n",
    "                coord = coord_rounded + 10\n",
    "            else:\n",
    "                coord = coord_rounded - 10\n",
    "    return coord\n",
    "\n",
    "# HELPER FUNCTION: Appends all nonCP regions used for conditional probabilites\n",
    "def append_nonCP_regions(allProb):\n",
    "    nonCP_regions = ['ACB','AST','CL','EN','LSX','PAL','PIR','SAMY']\n",
    "    for l in range(3):\n",
    "        if l == 0:\n",
    "            last_CP_col = 'CP_R'\n",
    "        elif l == 1:\n",
    "            last_CP_col = 'CPr_R'\n",
    "        else: # l == 2\n",
    "            last_CP_col = 'CPr_m_R'\n",
    "\n",
    "        for i,region in enumerate(nonCP_regions):\n",
    "            if i == 0:\n",
    "                prevCol = last_CP_col\n",
    "            else:\n",
    "                prevCol = f'{nonCP_regions[i-1]}{l}_R'\n",
    "\n",
    "            allProb.insert(allProb.columns.get_loc(prevCol)+1,f'{region}{l}_L',allProb[f'{region}_L'])\n",
    "            allProb.insert(allProb.columns.get_loc(f'{region}{l}_L')+1,f'{region}{l}_R',allProb[f'{region}_R'])\n",
    "    return allProb    \n",
    "\n",
    "# Absolute probabilities => (Normalize probabilities at multiple levels) => Conditional probabilities\n",
    "def normalize_prob(allProb):    \n",
    "    L0_labels = ['CP_L','CP_R','ACB0_L','ACB0_R','AST0_L','AST0_R','CL0_L','CL0_R','EN0_L','EN0_R','LSX0_L','LSX0_R','PAL0_L','PAL0_R','PIR0_L','PIR0_R','SAMY0_L','SAMY0_R']\n",
    "    allProb[L0_labels] = allProb[L0_labels].div(np.sum(allProb[L0_labels], axis=1), axis=0)\n",
    "\n",
    "    L1_labels = ['CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R','ACB1_L','ACB1_R','AST1_L','AST1_R','CL1_L','CL1_R','EN1_L','EN1_R','LSX1_L','LSX1_R','PAL1_L','PAL1_R','PIR1_L','PIR1_R','SAMY1_L','SAMY1_R']\n",
    "    allProb[L1_labels] = allProb[L1_labels].div(np.sum(allProb[L1_labels], axis=1), axis=0)\n",
    "    \n",
    "    L2_labels = ['CPc_d_L','CPc_d_R','CPc_i_L','CPc_i_R','CPc_v_L','CPc_v_R','CPi_dl_L','CPi_dl_R','CPi_dm_L','CPi_dm_R','CPi_vl_L','CPi_vl_R','CPi_vm_L','CPi_vm_R','CPr_imd_L','CPr_imd_R','CPr_imv_L','CPr_imv_R','CPr_l_L','CPr_l_R','CPr_m_L','CPr_m_R','ACB2_L','ACB2_R','AST2_L','AST2_R','CL2_L','CL2_R','EN2_L','EN2_R','LSX2_L','LSX2_R','PAL2_L','PAL2_R','PIR2_L','PIR2_R','SAMY2_L','SAMY2_R']\n",
    "    allProb[L2_labels] = allProb[L2_labels].div(np.sum(allProb[L2_labels], axis=1), axis=0)\n",
    "\n",
    "    nonCP_labels = ['ACB_L','ACB_R','AST_L','AST_R','CL_L','CL_R','EN_L','EN_R','LSX_L','LSX_R','PAL_L','PAL_R','PIR_L','PIR_R','SAMY_L','SAMY_R']\n",
    "    allProb[nonCP_labels] = allProb[nonCP_labels].div(np.sum(allProb[nonCP_labels], axis=1), axis=0)\n",
    "\n",
    "    multi_labels = ['CP_ACB', 'not_CP_ACB']\n",
    "    allProb[multi_labels] = allProb[multi_labels].div(np.sum(allProb[multi_labels], axis=1), axis=0)\n",
    "\n",
    "    return allProb\n",
    "\n",
    "# Convert distance transforms into probability distributions\n",
    "def dist_transform_to_prob_dist(dist, scale_factor = 10):\n",
    "    transformedMask = np.exp(-dist/scale_factor)\n",
    "    transformedMask = transformedMask / np.sum(transformedMask)\n",
    "    return transformedMask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e8aef-a09f-4fd6-a454-6210d4511fc7",
   "metadata": {},
   "source": [
    "### Postprocessing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199635b4-6038-478e-b937-259b139c3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to remove the .ipynb_checkpoints file from lists\n",
    "def brain_filter(dirName):\n",
    "    if dirName == '.ipynb_checkpoints' or dirName == 'TME20-1':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Return a pd.DataFrame (1 x len (df)) of the column name in df containing the corresponding value in vals for each row of df\n",
    "def xlookup(vals, df, cName = ''):\n",
    "    out = list()\n",
    "    for row in df.iterrows():\n",
    "        idx, row = row\n",
    "        val = vals[idx]\n",
    "        colName = row[row == val].keys()[0]\n",
    "        out.append(colName)\n",
    "    return pd.DataFrame(data=out, columns = [cName])\n",
    "\n",
    "# Return a pd.Series (1 x len(df)) of the nth largest value in each row of df\n",
    "def nmax(df, n):\n",
    "    out = list()\n",
    "    for row in df.iterrows():\n",
    "        idx, row = row\n",
    "        nth_largest = sorted(row, reverse=True)[n-1]\n",
    "        out.append(nth_largest)\n",
    "    return pd.Series(data=out)\n",
    "\n",
    "# \n",
    "def compute_top_n_maximums(data, all_labels, n=3):\n",
    "    \n",
    "    for i, labels in enumerate(all_labels):\n",
    "        L0_df = data[labels]\n",
    "        \n",
    "        L0_P1 = L0_df.max(axis=1)\n",
    "        L0_1 = xlookup(L0_P1, L0_df)\n",
    "    \n",
    "        if i == 0:\n",
    "            leftmost_col_name = 'fname'\n",
    "        else:\n",
    "            leftmost_col_name = f'P(L{i-1},3)'\n",
    "        \n",
    "        data.insert(data.columns.get_loc(leftmost_col_name)+1,f'L{i} (1)',L0_1)\n",
    "        data.insert(data.columns.get_loc(f'L{i} (1)')+1,f'P(L{i},1)',L0_P1)\n",
    "        \n",
    "        L0_P2 = nmax(L0_df, 2)\n",
    "        L0_2 = xlookup(L0_P2, L0_df)\n",
    "        data.insert(data.columns.get_loc(f'P(L{i},1)')+1,f'L{i} (2)',L0_2)\n",
    "        data.insert(data.columns.get_loc(f'L{i} (2)')+1,f'P(L{i},2)',L0_P2)\n",
    "        \n",
    "        L0_P3 = nmax(L0_df,3)\n",
    "        L0_3 = xlookup(L0_P3, L0_df)\n",
    "        data.insert(data.columns.get_loc(f'P(L{i},2)')+1,f'L{i} (3)',L0_3)\n",
    "        data.insert(data.columns.get_loc(f'L{i} (3)')+1,f'P(L{i},3)',L0_P3)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c2dea-2f27-4886-9d0b-70f7895d36d6",
   "metadata": {},
   "source": [
    "## Generate probability distributions, assign labels to neurons, and save csv files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cdf9c-f49f-4caa-87ee-b1b8184e865e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# brains = sorted(filter(brain_filter, os.listdir('/home/abenneck/dragonfly_work/dragonfly_outputs/')))\n",
    "brains = ['TME07-1', 'TME08-1', 'TME09-1', 'TME10-1', 'TME10-3']\n",
    "# brains = ['hTME15-1', 'hTME15-2', 'hTME18-1', 'hTME19-2']\n",
    "# brains = ['TME08-1', 'TME09-1']\n",
    "\n",
    "allRegionCoords = get_coord_list(672, XS, S) # 672 is regionID for CP\n",
    "allRegionCoords = np.asarray(allRegionCoords)\n",
    "uniform_prior = 1/100\n",
    "scale_factor = 5\n",
    "\n",
    "print('Loading distance transform distributions')\n",
    "start = time.time()\n",
    "# Create list of distributions from presaved distance transforms\n",
    "paramDir = '/home/abenneck/dragonfly_work/distance_parameters'\n",
    "L1_files = sorted(filter(L1_filter_dist,os.listdir(paramDir)))\n",
    "L2_files = sorted(filter(L2_filter_dist,os.listdir(paramDir)))\n",
    "nonCP_files = sorted(filter(non_CP_filter_dist,os.listdir(paramDir)))\n",
    "multi_files = ['cp_acb_dist_param.npz','not_cp_acb_dist_param.npz']\n",
    "\n",
    "allTransforms = list()\n",
    "for fileList in [L1_files, L2_files, nonCP_files, multi_files]:\n",
    "    for file in fileList:\n",
    "        data = np.load(os.path.join(paramDir,file))\n",
    "        distTrans = data['mask']\n",
    "        probDist = dist_transform_to_prob_dist(distTrans, scale_factor = scale_factor)\n",
    "        allTransforms.append(probDist)\n",
    "        print(file)\n",
    "print(f'Finished loading distributions in {time.time() - start} s\\n')\n",
    "\n",
    "# if True:\n",
    "#     raise Exception('Rewrite some of the below code')\n",
    "\n",
    "print('Generating probabilities')\n",
    "start = time.time()\n",
    "# Generate probabilities\n",
    "for brain in brains:\n",
    "    # Define relevant directories\n",
    "    neuronDir = f'/home/abenneck/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs/'\n",
    "\n",
    "    # Note: The ACB#_x columns are inserted after computing absolute probabilities, but before normalization\n",
    "    L0_col = ['mouse ID', 'slice', 'hemi', 'neuron ID','fname','CP_L','CP_R']\n",
    "    L1_col = ['CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R',]\n",
    "    L2_col = ['CPc_d_L','CPc_d_R','CPc_i_L','CPc_i_R','CPc_v_L','CPc_v_R','CPi_dl_L','CPi_dl_R','CPi_dm_L','CPi_dm_R','CPi_vl_L','CPi_vl_R','CPi_vm_L','CPi_vm_R','CPr_imd_L','CPr_imd_R','CPr_imv_L','CPr_imv_R','CPr_l_L','CPr_l_R','CPr_m_L','CPr_m_R']\n",
    "    nonCP_col = ['ACB_L','ACB_R','AST_L','AST_R','CL_L','CL_R','EN_L','EN_R','LSX_L','LSX_R','PAL_L','PAL_R','PIR_L','PIR_R','SAMY_L','SAMY_R']\n",
    "    multi_col = ['CP_ACB', 'not_CP_ACB']\n",
    "    allCol = np.concatenate([L0_col, L1_col, L2_col, nonCP_col, multi_col])\n",
    "    allProb = pd.DataFrame(columns=allCol)\n",
    "    \n",
    "    # Generate regional probabilities for every neuron in neuronDir\n",
    "    row_idx = 0\n",
    "    for i, file in enumerate(sorted(os.listdir(neuronDir))):\n",
    "        if \"mapped.swc\" in file and 'checkpoint' not in file:\n",
    "            # Load soma coordinates\n",
    "            data = pd.read_csv(os.path.join(neuronDir,file))\n",
    "            x = float(data.columns[2])\n",
    "            y = float(data.columns[3])\n",
    "            z = float(data.columns[4])\n",
    "            soma_location = [x,y,z]\n",
    "\n",
    "            # Get idx for soma location in xS\n",
    "            x_ind = np.where(xS[0] == roundCoord(xS[0], x))[0].item()\n",
    "            y_ind = np.where(xS[1] == roundCoord(xS[1], y))[0].item() \n",
    "            z_ind = np.where(xS[2] == roundCoord(xS[2], z))[0].item()\n",
    "\n",
    "            if 'hTME' in brain:\n",
    "                mouseID = file.split('_')[0]\n",
    "                sliceID = file.split('_')[2]\n",
    "                hemiID = 'N/A'\n",
    "                neuronID = 'N/A'\n",
    "            else:\n",
    "                # Get metadata from filename\n",
    "                mouseID = file.split('_')[1]\n",
    "                sliceID = file.split('_')[4][:-1]\n",
    "                hemiID = file.split('_')[4][-1]\n",
    "                neuronID = file.split('_')[0]\n",
    "    \n",
    "            # Compute probabilities from distance transform probabilities\n",
    "            # cpDist = dist_from_region(allRegionCoords,soma_location)\n",
    "            neuronProb = [mouseID, sliceID, hemiID, neuronID, file]\n",
    "            for transform in allTransforms:\n",
    "                neuronProb.append(uniform_prior*transform[0, x_ind, y_ind, z_ind])\n",
    "\n",
    "            allProb.loc[row_idx] = neuronProb\n",
    "            row_idx+=1\n",
    "            print(f'{i}/{len(sorted(os.listdir(neuronDir)))} Finished {file}')\n",
    "\n",
    "    # Insert Non-CP regions at each level\n",
    "    allProb = append_nonCP_regions(allProb)\n",
    "\n",
    "    # Normalize probabilities at each level\n",
    "    allProb = normalize_prob(allProb)\n",
    "\n",
    "    # Identify top 3 regions for each neuron and insert columns into allProb\n",
    "    L0_labels = ['CP_L','CP_R','ACB0_L','ACB0_R','AST0_L','AST0_R','CL0_L','CL0_R','EN0_L','EN0_R','LSX0_L','LSX0_R','PAL0_L','PAL0_R','PIR0_L','PIR0_R','SAMY0_L','SAMY0_R']\n",
    "    L1_labels = ['CPc_L','CPc_R','CPi_L','CPi_R','CPr_L','CPr_R','ACB1_L','ACB1_R','AST1_L','AST1_R','CL1_L','CL1_R','EN1_L','EN1_R','LSX1_L','LSX1_R','PAL1_L','PAL1_R','PIR1_L','PIR1_R','SAMY1_L','SAMY1_R']\n",
    "    L2_labels = ['CPc_d_L','CPc_d_R','CPc_i_L','CPc_i_R','CPc_v_L','CPc_v_R','CPi_dl_L','CPi_dl_R','CPi_dm_L','CPi_dm_R','CPi_vl_L','CPi_vl_R','CPi_vm_L','CPi_vm_R','CPr_imd_L','CPr_imd_R','CPr_imv_L','CPr_imv_R','CPr_l_L','CPr_l_R','CPr_m_L','CPr_m_R','ACB2_L','ACB2_R','AST2_L','AST2_R','CL2_L','CL2_R','EN2_L','EN2_R','LSX2_L','LSX2_R','PAL2_L','PAL2_R','PIR2_L','PIR2_R','SAMY2_L','SAMY2_R']\n",
    "    multi_labels = ['CP_ACB', 'not_CP_ACB'] # No normalization, so omit from all_labels\n",
    "    all_labels = [L0_labels, L1_labels, L2_labels]\n",
    "\n",
    "    # Appends columns containing top 3 regions (+ associated probability) with highest probability at L0, L1, and L2 for each neuron\n",
    "    allProb = compute_top_n_maximums(allProb, all_labels)\n",
    "    \n",
    "    # Save probability distributions as one .csv file in tempDir\n",
    "    tempDir = f'/home/abenneck/dragonfly_work/prob_dist_out/{brain}_neuron_region_prob_sf{scale_factor}.csv'   \n",
    "    allProb.to_csv(tempDir, index=False)\n",
    "\n",
    "    print(f'Finished {brain} in {time.time() - start} s with an average of {(time.time() - start) / len(allProb)} s / neuron\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85f66b-80f4-4928-9000-bbb0c86d7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adea7b3-a4b6-40f0-8f50-a3189ccac886",
   "metadata": {},
   "source": [
    "# 2. Merge csv files with (position, probability, morphometric features) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d7fad-0184-410d-afcc-cf74e6f9dc15",
   "metadata": {},
   "source": [
    "## Merge probability csv files from all brains into one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c85a7-46c2-4e07-be83-7ddace3f9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 5\n",
    "brains = ['TME07-1', 'TME08-1', 'TME09-1', 'TME10-1', 'TME10-3', 'hTME15-1', 'hTME15-2', 'hTME18-1', 'hTME19-2']\n",
    "\n",
    "all_df = list()\n",
    "for brain in brains:\n",
    "    fname = f'/home/abenneck/dragonfly_work/prob_dist_out/{brain}_neuron_region_prob_sf{scale_factor}.csv'\n",
    "    data = pd.read_csv(fname)\n",
    "    all_df.append(data)\n",
    "\n",
    "main_data = pd.concat(all_df)\n",
    "main_data.to_csv(f'/home/abenneck/dragonfly_work/prob_dist_out/neuron_region_prob_sf{scale_factor}.csv')\n",
    "np.unique(main_data['mouse ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c8a11-6950-477e-bcad-c158f6293eff",
   "metadata": {},
   "source": [
    "## Generate + save csv containing neuron soma locations, key_name, and slice info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce68e2-8c0f-4563-a166-c0fdbbfe6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soma_from_fname(fname, brain):\n",
    "    '''\n",
    "    load an swc file => soma location derived from fname\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        A string pointing to the file to load\n",
    "    skip : int\n",
    "        Number of lines to skip at the beginning. (default 0)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xyzoff : Array of ints\n",
    "        the x, y, z offsets between fname and first line\n",
    "    \n",
    "    '''\n",
    "    if 'hTME' in brain or brain in ['Hpca5-2', 'Sp9-3-2', 'Trank1-2-3', 'Zswim4-1', 'MQC06-2', 'MQC09-3', 'MQC18-3', 'MQC82-2']:\n",
    "        if 'Unmapped' in fname: # neuron_dir is a global variable defined in one of the first cells\n",
    "            print(fname)\n",
    "            _, fx, fy, fz = ((fname.split('/'))[-1].split('_')[-1]).split('-')\n",
    "            fz = fz[:-4]\n",
    "        else:\n",
    "            print(fname)\n",
    "            _, fx, fy, fz = fname.split('_')[3].split('-')\n",
    "        \n",
    "        fx, fy, fz = float(fx), float(fy), float(fz)\n",
    "    elif brain == 'TME10-3':\n",
    "        fx, fy, fz = fname.split('_')[5:-1]\n",
    "        fx, fy, fz = fx[1:], fy[1:], fz[1:]\n",
    "    elif 'TME' in brain:\n",
    "        fx, fy, fz = fname.split('_')[5:-2]\n",
    "        fx, fy, fz = fx[1:], fy[1:], fz[1:]\n",
    "    else:\n",
    "        raise Exception(f'Invalid brain ({brain}) supplied when calling get_soma_from_fname()')\n",
    "\n",
    "    return np.array([int(fx), int(fy), int(fz)])\n",
    "\n",
    "def get_soma_from_first_line(fname, brain):\n",
    "    ''' load an swc file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        A string pointing to the file to load\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V : numpy array of float\n",
    "        An array storing vertices\n",
    "    E : numpy array of int\n",
    "        An array storing edges.  they are ordered as parent -> this sample\n",
    "    R : numpy array of float\n",
    "        An arrow storing radii\n",
    "    \n",
    "    '''       \n",
    "\n",
    "    with open(fname,'rt') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            # print(f'{i}: {line}')\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "                \n",
    "            # print(line)\n",
    "            data = line.split()\n",
    "            # may be comma separated\n",
    "            if len(data)==1:\n",
    "                data = line.split(',')\n",
    "            sample_number = int(data[0])\n",
    "            x = float(data[2])\n",
    "            y = float(data[3])\n",
    "            z = float(data[4])\n",
    "            break\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128da948-9193-4a20-a52b-8220a4bdf0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saveAllPos = False\n",
    "\n",
    "brains = ['TME07-1', 'TME08-1', 'TME09-1', 'TME10-1', 'TME10-3', 'hTME15-1', 'hTME15-2', 'hTME18-1', 'hTME19-2']\n",
    "\n",
    "allCol = ['fname','key_name','x','y','z','slice','hemi']\n",
    "# allCol = ['fname','x','y','z','slice','hemi']\n",
    "df_out = pd.DataFrame(columns=allCol)\n",
    "\n",
    "row_idx = 0\n",
    "\n",
    "start = time.time()\n",
    "for brain in brains:\n",
    "    \n",
    "    # Define relevant input directory\n",
    "    neuronDir = f'/home/abenneck/nafs/dtward/andrew_work/test/dragonfly_work/dragonfly_outputs/{brain}/dragonfly_joint_outputs/'\n",
    "\n",
    "    for i, file in enumerate(sorted(os.listdir(neuronDir))):\n",
    "        if \"mapped.swc\" in file and 'checkpoint' not in file:\n",
    "            # Load soma coordinates\n",
    "            data = pd.read_csv(os.path.join(neuronDir,file))\n",
    "            # x, y, z = get_soma_from_fname(file, brain)\n",
    "            x, y, z = get_soma_from_first_line(os.path.join(neuronDir,file), brain)\n",
    "\n",
    "            # Get metadata from filename and initialize row\n",
    "            if 'hTME' in brain:\n",
    "                sliceID = file.split('_')[2]\n",
    "                hemiID = 'N/A'\n",
    "            else: # TME brains\n",
    "                sliceID = file.split('_')[4][:-1]\n",
    "                hemiID = file.split('_')[4][-1]\n",
    "\n",
    "            # Generate key from 'file' based on pattern in Ming's csv\n",
    "            if 'hTME' in brain:\n",
    "                strOut = file[:-22]\n",
    "            else:\n",
    "                strSplits = file.split('_')\n",
    "                strOut = f'{strSplits[0]}_{strSplits[1]}_{strSplits[2]}_{strSplits[3]}_{strSplits[4]}_xyz-{strSplits[5][1:]}-{strSplits[6][1:]}-{strSplits[7][1:]}'\n",
    "            print(strOut)\n",
    "            \n",
    "            # row = [file, strOut, x, y, z, sliceID, hemiID, dist_cp, dist_lv, dist_3V, dist_Aq, dist_4V, dist_centCan, dist_cc, dist_brain_bound]\n",
    "            row = [file, strOut, x, y, z, sliceID, hemiID]\n",
    "            # row = [file, x, y, z, sliceID, hemiID]\n",
    "            df_out.loc[row_idx] = row\n",
    "            row_idx+=1\n",
    "            print(f'{i}/{len(sorted(os.listdir(neuronDir)))} Finished {file}\\n')\n",
    "\n",
    "    # print(f'Finished {brain} in {time.time() - start} s with an average of {(time.time() - start) / len(allProb)} s / neuron\\n')\n",
    "    print(f'Finished {brain} in {time.time() - start} s')\n",
    "\n",
    "tempDir = f'/home/abenneck/dragonfly_work/prob_dist_out/all_pos.csv'\n",
    "if saveAllPos:\n",
    "    df_out.to_csv(tempDir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a9286-2792-4384-a6cb-01c61a1a2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception('Check before merging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38505cd-7917-4ccc-8acd-69b50ad63d4d",
   "metadata": {},
   "source": [
    "## Load 3 morphometric feature csv files + combine into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae3b63-c5c9-47dc-a631-a93211a9ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TME_morpho_path = f'/home/abenneck/dragonfly_work/morpho_data/d1d2_subclusters_Dec-14-2023_share.csv'\n",
    "hTME_HD_morpho_path = f'/home/abenneck/dragonfly_work/morpho_data/d1d2_with_clusters_2hTME_HD_results.by.Brain.Adjusted-Dec-21-2023.csv'\n",
    "hTME_WT_morpho_path = f'/home/abenneck/dragonfly_work/morpho_data/d1d2_with_clusters_2hTME_WT_results.by.Brain.Adjusted-Dec-21-2023.csv'\n",
    "\n",
    "TME_morpho = pd.read_csv(TME_morpho_path)\n",
    "hTME_HD_morpho = pd.read_csv(hTME_HD_morpho_path)\n",
    "hTME_WT_morpho = pd.read_csv(hTME_WT_morpho_path)\n",
    "\n",
    "all_morpho = pd.concat([TME_morpho, hTME_HD_morpho, hTME_WT_morpho])\n",
    "print(f'{len(all_morpho)} neurons in morpho.csv')\n",
    "\n",
    "saveMorpho = False\n",
    "\n",
    "if saveMorpho:\n",
    "    all_morpho.to_csv('/home/abenneck/dragonfly_work/morpho_data/all_morpho.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226416c5-921a-4bb3-bd8f-1c603054c129",
   "metadata": {},
   "source": [
    "## Merge position and probability csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef294c66-c229-4ed6-93eb-1751a7fe159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = '/home/abenneck/dragonfly_work/prob_dist_out/all_pos.csv'\n",
    "all_pos = pd.read_csv(pos_path)\n",
    "\n",
    "print(f'{len(all_pos)} neurons in pos.csv')\n",
    "\n",
    "prob_path = '/home/abenneck/nafs/dtward/andrew_work/test/dragonfly_work/prob_dist_out/neuron_region_prob_sf5.csv'\n",
    "all_prob = pd.read_csv(prob_path)\n",
    "\n",
    "print(f'{len(all_prob)} neurons in prob.csv')\n",
    "\n",
    "# (02/22/24) Confirmed that all_pos['fname'] == all_prob['fname']\n",
    "pos_and_prob = all_pos.merge(all_prob, how = 'outer')\n",
    "\n",
    "if False:\n",
    "    pos_and_prob.to_csv('/home/abenneck/dragonfly_work/prob_dist_out/all_pos_and_prob.csv',index = False)\n",
    "\n",
    "print(f'{len(pos_and_prob)} neurons in all_pos_and_prob.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefe967-632b-46b1-a803-4e0f3dd3d3b5",
   "metadata": {},
   "source": [
    "## Merge pos_and_prob with morpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c6104-a20b-4b2d-899e-0ea5fa00c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = all_morpho.merge(pos_and_prob, left_on = 'file_path', right_on = 'key_name', how = 'outer')\n",
    "out_df = out_df.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "out_df.to_csv('/home/abenneck/dragonfly_work/prob_dist_out/pos_prob_morpho.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea79ff0-3557-480f-8c2e-be309874ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_col = all_morpho['file_path']\n",
    "pp_col = pos_and_prob['key_name']\n",
    "merged_col = out_df['key_name']\n",
    "\n",
    "in_morpho_out_pp = morpho_col[~morpho_col.index.isin(pos_and_prob.index)]      # len() = 0\n",
    "in_pp_out_morpho = pp_col[~pp_col.index.isin(all_morpho.index)]                # len() = 0\n",
    "\n",
    "len(in_morpho_out_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe1c85-8e3e-44fc-b664-2b4d8833b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hTME = pd.read_csv('/home/abenneck/dragonfly_work/morpho_data/complete_neurons_hTME.csv')\n",
    "all_TME = pd.read_csv('/home/abenneck/dragonfly_work/morpho_data/complete_neurons_TME.csv')\n",
    "all_neuron = pd.concat([all_hTME, all_TME])\n",
    "\n",
    "out_df = all_pos.merge(all_neuron, right_on = 'file_path', left_on = 'key_name', how = 'inner')\n",
    "out_df.to_csv('/home/abenneck/dragonfly_work/all_pos_valRecon.csv', index = False)\n",
    "\n",
    "# excluded_neurons = pd.read_csv('/home/abenneck/dragonfly_work/morpho_data/excluded_neurons.csv')\n",
    "\n",
    "# len(all_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a604b5-6fe7-4a88-a837-89afa97aacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e26d98-8fe9-44c4-8a4b-d5018279393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2accbcb-f55d-4ff5-ac7e-27a427f78e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0dd4a-817a-48b3-b18d-eb5f97296981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df_inner : The csv used for generating the 3d neuronal distributions\n",
    "# merged_prob_df : The csv containing the soma coords and probability values\n",
    "\n",
    "\n",
    "# For merging the prob and dist csv files\n",
    "# morpho_path = '/home/abenneck/dragonfly_work/d1d2_with_clusters_Nov-9-2023.csv'\n",
    "# morpho_df = pd.read_csv(morpho_path)\n",
    "\n",
    "dist_path = '/home/abenneck/dragonfly_work/position_and_dist.csv'\n",
    "dist_df = pd.read_csv(dist_path)\n",
    "\n",
    "merged_df_inner = dist_df.merge(morpho_df, right_on = 'file path', left_on = 'key_name', how = 'inner')\n",
    "\n",
    "prob_path = '/home/abenneck/nafs/dtward/andrew_work/test/dragonfly_work/prob_dist_out/neuron_region_prob_sf5.csv'\n",
    "prob_df = pd.read_csv(prob_path)\n",
    "\n",
    "merged_prob_df = merged_df_inner.merge(prob_df, right_on = 'fname', left_on = 'fname', how = 'inner')\n",
    "merged_prob_df = merged_prob_df.drop(labels = list(merged_prob_df.columns[5:57]), axis=1) # Drop all thr morpho features\n",
    "merged_prob_df = merged_prob_df.rename(columns = {'slice_y' : 'slice', 'hemi_y' : 'hemi'})\n",
    "\n",
    "merged_prob_df.head()\n",
    "\n",
    "# merged_prob_df.to_csv('/home/abenneck/dragonfly_work/prob_dist_out/pos_and_prob.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9603e4-679f-4843-8f79-51c81534b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'morpho: {len(morpho_df)}, dist: {len(dist_df)}, merged: {len(merged_df_inner)}, prob: {len(prob_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1365de-0bb5-4a21-98a9-a9e1c1129a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the hTME prob and pos csv files\n",
    "prob_df = pd.read_csv('/home/abenneck/dragonfly_work/prob_dist_out/neuron_region_prob_sf5.csv')\n",
    "pos_df = pd.read_csv('/home/abenneck/dragonfly_work/prob_dist_out/all_position_and_dist.csv')\n",
    "\n",
    "out_df = prob_df.merge(pos_df, how = 'inner')\n",
    "out_df = out_df.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# Converts 'neuron ID' from float to int (All neuron ID just have a '.0' after)\n",
    "out_df['neuron ID'] = out_df['neuron ID'].astype('Int64')\n",
    "\n",
    "out_df.to_csv('/home/abenneck/dragonfly_work/prob_dist_out/all_pos_and_prob.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413a89-4977-4782-b2c3-93f818bdf916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morpho_col = morpho_df['file path']\n",
    "dist_col = dist_df['key_name']\n",
    "merged_col = merged_df_inner['key_name']\n",
    "\n",
    "in_morpho_out_dist = morpho_col[~morpho_col.index.isin(dist_col.index)]     # len() = 0\n",
    "in_morpho_out_merged = morpho_col[~morpho_col.index.isin(merged_col.index)] # len() == 43\n",
    "\n",
    "in_dist_out_morpho = dist_col[~dist_col.index.isin(morpho_col.index)]       # len() == 6\n",
    "in_dist_out_merged = dist_col[~dist_col.index.isin(merged_col.index)]       # len() == 49\n",
    "\n",
    "in_merged_out_dist = merged_col[~merged_col.index.isin(dist_col.index)]     # len() == 0\n",
    "in_merged_out_morpho = merged_col[~merged_col.index.isin(morpho_col.index)] # len() == 0\n",
    "\n",
    "# in_morpho_out_dist\n",
    "# in_morpho_out_merged\n",
    "\n",
    "# in_dist_out_morpho\n",
    "# in_dist_out_merged\n",
    "\n",
    "# in_merged_out_dist\n",
    "# in_merged_out_morpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f0c0b-1d2a-462b-abc3-cb264d5e9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = merged_df[merged_df['_merge'] != 'both']\n",
    "\n",
    "dfTest = missing_df[['file path', 'key_name','fname', '_merge']]\n",
    "dfTest.to_csv('/home/abenneck/dragonfly_work/merged_diff_Nov2023.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7432418-ab11-4894-abf2-5c67d1b55814",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('End of postprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2874d-1086-49f4-88c5-160275920345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_columns', 10)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
